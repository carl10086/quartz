
## Refer

- [KServe](https://www.kubeflow.org/docs/external-add-ons/kserve/)
- [Serving-Runtime](https://kserve.github.io/website/master/modelserving/v1beta1/serving_runtime/)


## 1-Quick start


![](https://imgs-1322738462.cos.ap-shanghai.myqcloud.com/202408211837291.png)



## 2-HuggingfaceServer


**1) Deploy HuggingFace Server on KServe**



å‰©ä¸‹çš„ `resources` åˆ™æ˜¯é€šç”¨çš„è®¾ç½®.

```sh
curl -H "content-type:application/json" -v localhost:8080/openai/v1/chat/completions -d '{"model": "qwen15-14b", "messages": [{"role": "user","content": "æˆ‘çš„å­©å­æ•°å­¦ä¸å¤ªè¡Œï¼Œæ€ä¹ˆåŠæ€ä¹ˆåŠ"}], "stream":false }'
```


å¯èƒ½å¾—é…ç½®:

```yaml
apiVersion: "serving.kserve.io/v1beta1"
kind: "InferenceService"
metadata:
  name: "llama2-7b-chat"
  namespace: "${YOUR-NS}"
spec:
  predictor:
    model:
      modelFormat:
        name: huggingface
      args:
        - "--model_dir=/mnt/models/Llama-2-7b-chat-hf"
        - "--model_name=llama2"
      storageUri: "pvc://global-shared/models"
      resources:
        limits:
          cpu: "6"
          memory: 24Gi
          nvidia.com/gpu: "1"
        requests:
          cpu: "6"
          memory: 24Gi
          nvidia.com/gpu: "1"
```


æ ¸å¿ƒé…ç½®å¦‚ä¸‹:

- `modelFormat`:  `huggingface` , è¿™å°±å‘Šè¯‰äº†é•œåƒå¯åŠ¨çš„æ—¶å€™ç”¨ `huggingfaceserver` å»å¼€å¯æ¨ç†æœåŠ¡
- ç„¶å `args` çš„å‚æ•°åˆ™æ˜¯ `huggingfaceserver` å­æ¨¡å—å¯åŠ¨çš„å‚æ•°.
	- æ”¯æŒçš„å‚æ•° ï¼Œå¯ä»¥ç›´æ¥çœ‹ [æºç ](https://github.com/kserve/kserve/blob/master/python/huggingfaceserver/huggingfaceserver/__main__.py)


é¢å¤–çš„:

â€¢ `SAFETENSORS_FAST_GPU` è¢«é»˜è®¤è®¾ç½®ï¼Œä»¥æé«˜æ¨¡å‹åŠ è½½æ€§èƒ½ã€‚
â€¢ `HF_HUB_DISABLE_TELEMETRY` è¢«é»˜è®¤è®¾ç½®ï¼Œä»¥ç¦ç”¨é¥æµ‹



**2) è§£é‡Šä¸€ä¸‹ä¸Šé¢çš„å­˜å‚¨ä½ç½®**

ä¸€èˆ¬åœ¨å¤–ç½‘æŒ‡å®šä¸€ä¸ª `model_id` å°±å¯ä»¥.  è¿™é‡Œæ˜¯ç¦»çº¿ç‰ˆæœ¬.

é¦–å…ˆ, `storageUri: "pvc://global-shared/models"` ä¼šè¢«æŒ‚è½½åˆ° `/mnt/models` ç›®å½•ï¼Œä¸ç®¡å†™ä»€ä¹ˆéƒ½ä¼šæŒ‚è½½åˆ° `/mnt/models` ç›®å½•. ç¡¬ç¼–ç å†™æ­»åœ¨ ä»£ç  [storage.py](https://github.com/kserve/kserve/blob/master/python/kserve/kserve/storage/storage.py) ä¸­.

æ‰€ä»¥æˆ‘ä»¬è¦ç¡®ä¿:

- åœ¨ `pvc` å·ä¸‹é¢æœ‰è¿™ä¸ªç›®å½• `Llama-2-7b-chat-hf` .
- è¿™ä¸ªç›®å½•å¿…é¡»ç¬¦åˆ `huggingface` çš„ `model` æ ¼å¼ï¼Œæœ‰ä¸€ä¸ª `config.json` èƒ½ä»£è¡¨ä»–æ‰€æœ‰çš„å…ƒæ•°æ®.
- ç¡®ä¿ `pvc` å·ä¸­ `Llama-2-7b-chat-hf` è¿™ä¸ªç›®å½•çš„æƒé™æœ‰ `755` å¯è¯»å–ï¼Œéœ€è¦å¤åˆ¶åˆ°å®¹å™¨å†…éƒ¨


**3) test**

```sh
## 1. å…ˆä¸´æ—¶è½¬å‘ç»•è¿‡ ingress æƒé™æ ¡éªŒ
kubectl port-forward -n .... pod/huggingface-qwen15-14b-predictor-00001-deployment-6f7f8c45h7n85 8080:8080

## 2. 
root@gpu7:~# curl -H "content-type:application/json"  localhost:8080/openai/v1/completions -d '{"model": "llama2", "prompt": "ä½ å¥½! æˆ‘ä»¬ç”¨ä¸­æ–‡æ‰“ä¸ªæ‹›å‘¼", "stream":false, "max_tokens": 30 }'
{"id":"cmpl-b00630ab36db4fe5833632c5a5dd3322","choices":[{"finish_reason":"length","index":0,"logprobs":null,"text":"ã€‚ Hi there! We're using Chinese to greet you. ğŸ˜ŠğŸ‡¨ğŸ‡³\n\nåœ¨"}],"created":1724299244,"model":"llama2","system_fingerprint":null,"object":"text_completion","usage":{"completion_tokens":30,"prompt_tokens":19,"total_tokens":49}
```




## 3-Custom Image

```dockerfile
ARG PYTHON_VERSION=3.9
ARG BASE_IMAGE=python:${PYTHON_VERSION}-slim-bullseye

FROM ${BASE_IMAGE} as builder

COPY pip.conf /etc/pip.conf

RUN useradd kserve -m -u 1000 -d /home/kserve
USER 1000

RUN pip install vllm
```


è¿™ä¸ªå§¿åŠ¿ï¼Œå‚è€ƒäº† é˜¿é‡Œçš„ KServe çš„éƒ¨ç½²æ–¹å¼ï¼Œæ„Ÿè§‰å¾ˆçµæ´»ï¼Œå¾ˆèˆ’æœï¼Œè¿™é‡Œéšä¾¿ç”¨ä¸€ä¸ªé•œåƒæµ‹è¯•ä¸€ä¸‹.

```yaml
apiVersion: "serving.kserve.io/v1beta1"
kind: "InferenceService"
metadata:
  name: "ysz-t1"
  namespace: "Your-ns"
  annotations:
    "sidecar.istio.io/inject": "false"
spec:
  predictor:
    containers:
      - name: kserve-vllm
        image: kserve/vllm:v1.9.0-dirty
        imagePullPolicy: Always
        command: ["/bin/sh", "-c"]
        args: ["python3 -m vllm.entrypoints.openai.api_server --port 8080 --trust-remote-code --served-model-name qwen --model /mnt/models/Qwen1.5-14B-Chat"]
        env:
          - name: STORAGE_URI
            value: "pvc://global-shared/models"
        resources:
          limits:
            cpu: "6"
            memory: "24Gi"
            nvidia.com/gpu: "1"
          requests:
            cpu: "6"
            memory: "24Gi"
            nvidia.com/gpu: "1"
        readinessProbe:
          tcpSocket:
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
```


- æ²¡æœ‰ `model` å‚æ•°ï¼Œä½¿ç”¨ ç¯å¢ƒå˜é‡ä»£è¡¨èµ„æºä½ç½®.

```sh
curl -X POST "http://Your-service-url/v1/completions" \
-H "Content-Type: application/json" \
-H "Host: ...." \
-d '{"model": "qwen", "prompt": "ä½ å¥½! æˆ‘ä»¬ç”¨ä¸­æ–‡æ‰“ä¸ªæ‹›å‘¼", "stream": false, "max_tokens": 30}'
```

```json
{
  "id": "cmpl-8c7ac313bcd94833b46d40969bbb1d55",
  "object": "text_completion",
  "created": 1724329482,
  "model": "qwen",
  "choices": [
    {
      "index": 0,
      "text": "å§ã€‚ æ‚¨å¥½ï¼å¾ˆé«˜å…´ç”¨ä¸­æ–‡å’Œæ‚¨äº¤æµã€‚æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ",
      "logprobs": null,
      "finish_reason": "stop",
      "stop_reason": null
    }
  ],
  "usage": {
    "prompt_tokens": 10,
    "total_tokens": 29,
    "completion_tokens": 19
  }
}
```


## 4-Multiple Gpu card

```yaml
apiVersion: "serving.kserve.io/v1beta1"
kind: "InferenceService"
metadata:
  name: "qwen2-72b"
  namespace: "${Your-NS}"
  annotations:
    "sidecar.istio.io/inject": "false"
spec:
  predictor:
    volumes:
    - name: dshm
      emptyDir:
        medium: Memory
        sizeLimit: "10Gi"
    model:
      volumeMounts:
        - mountPath: /dev/shm
          name: dshm
          readOnly: false
      modelFormat:
        name: huggingface
      args:
        - "--model_dir=/mnt/models/Qwen2-72B-Instruct-GPTQ-Int4"
        - "--model_name=qwen2-72b"
        - "--max_length=6144"
        - "--trust-remote-code"
        - '--gpu-memory-utilization=0.95'
        - "--quantization gptq"
      storageUri: "pvc://global-shared/models"
      resources:
        limits:
          cpu: '24'
          memory: 300Gi
          nvidia.com/gpu: '2'
        requests:
          cpu: '24'
          memory: 300Gi
          nvidia.com/gpu: '2'
```