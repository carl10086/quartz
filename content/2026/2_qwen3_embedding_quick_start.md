

## refer
- [hugging_face](https://huggingface.co/Qwen/Qwen3-Embedding-8B)


## 1ï¸âƒ£ ä»€ä¹ˆæ˜¯ Qwen3 Embeddingï¼Ÿ

**ä¸“é—¨çš„æ–‡æœ¬åµŒå…¥æ¨¡å‹**ï¼Œç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡ï¼Œå®ç°è¯­ä¹‰æ£€ç´¢ã€‚

| è§„æ ¼      | å‚æ•°               |
| :------ | :--------------- |
| æ¨¡å‹ç³»åˆ—    | 0.6B, 4B, 8B     |
| å‘é‡ç»´åº¦    | 1024, 2560, 4096 |
| ä¸Šä¸‹æ–‡é•¿åº¦   | 32K tokens       |
| MTEB åˆ†æ•° | 70.58ï¼ˆ8Bï¼Œç¬¬ä¸€åï¼‰    |
| æ”¯æŒè¯­è¨€    | 100+             |

---

## 2ï¸âƒ£ Task Instruction æœºåˆ¶

### æ ¸å¿ƒæ¦‚å¿µ
```
Task = å‘Šè¯‰æ¨¡å‹"è¦åšä»€ä¹ˆä»»åŠ¡"
Query = ç”¨æˆ·çš„æœç´¢/é—®é¢˜ï¼ˆéœ€è¦åŠ  Taskï¼‰
Document = è¢«æœç´¢çš„å†…å®¹ï¼ˆä¸éœ€è¦åŠ  Taskï¼‰
```

### éå¯¹ç§°æ£€ç´¢
```python
# Query ç«¯ï¼šåŠ  Task
query = "Instruct: Given a web search query, retrieve relevant passages
         Query: What is the capital of China?"

# Document ç«¯ï¼šä¸åŠ  Task
document = "The capital of China is Beijing."
```

**åŸå› **ï¼š
- Query çŸ­å°æ¨¡ç³Šï¼Œéœ€è¦ä¸Šä¸‹æ–‡
- Document å†…å®¹å®Œæ•´ï¼Œä¸éœ€è¦é¢å¤–è¯´æ˜

**æ•ˆæœ**ï¼šç²¾åº¦æå‡ 3-5%

---

## 3ï¸âƒ£ ä¸¤ç§ä½¿ç”¨æ–¹å¼

### æ–¹å¼1ï¼šsentence-transformersï¼ˆæ¨èï¼‰

```python
from sentence_transformers import SentenceTransformer

# åŠ è½½æ¨¡å‹
model = SentenceTransformer("Qwen/Qwen3-Embedding-8B")

# Query åŠ  prompt
query_embeddings = model.encode(
    ["What is the capital of China?"],
    prompt_name="query"  # ä½¿ç”¨å†…ç½® query prompt
)

# Document ä¸åŠ  prompt
doc_embeddings = model.encode([
    "The capital of China is Beijing."
])

# è®¡ç®—ç›¸ä¼¼åº¦
similarity = model.similarity(query_embeddings, doc_embeddings)
# è¾“å‡º: [[0.7493]]
```

### æ–¹å¼2ï¼švLLMï¼ˆé«˜æ€§èƒ½ï¼‰

```python
from vllm import LLM
import torch

# åŠ è½½æ¨¡å‹
model = LLM(model="Qwen/Qwen3-Embedding-8B", task="embed")

# æ„é€  Task Instruction
def get_instruct(task, query):
    return f'Instruct: {task}\nQuery:{query}'

task = 'Given a web search query, retrieve relevant passages'

# Query åŠ  task
queries = [
    get_instruct(task, 'What is the capital of China?'),
    get_instruct(task, 'Explain gravity')
]

# Document ä¸åŠ  task
documents = [
    "The capital of China is Beijing.",
    "Gravity is a force that attracts..."
]

# æ‰¹é‡ç¼–ç 
outputs = model.embed(queries + documents)
embeddings = torch.tensor([o.outputs.embedding for o in outputs])

# è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
scores = embeddings[:2] @ embeddings[2:].T
# è¾“å‡º: [[0.748, 0.076], [0.089, 0.630]]
```

---

## 4ï¸âƒ£ å®æˆ˜ï¼šè®°å¿†ç®¡ç†ç³»ç»Ÿ

```python
class MemorySystem:
    def __init__(self):
        self.model = LLM(model="Qwen/Qwen3-Embedding-8B", task="embed")
    
    def store_memory(self, text: str, user_id: str):
        """å­˜å‚¨è®°å¿† - ä¸åŠ  task"""
        embedding = self.model.embed([text])[0].outputs.embedding
        vector_db.insert(
            embedding=embedding,
            text=text,
            user_id=user_id
        )
    
    def search_memory(self, query: str, user_id: str):
        """æœç´¢è®°å¿† - åŠ  task"""
        task = "Given a user query, retrieve relevant memories"
        query_with_task = f"Instruct: {task}\nQuery: {query}"
        
        query_embedding = self.model.embed([query_with_task])[0].outputs.embedding
        
        results = vector_db.search(
            embedding=query_embedding,
            filter={"user_id": user_id},
            top_k=5
        )
        return results

# ä½¿ç”¨
memory = MemorySystem()

# å­˜å‚¨
memory.store_memory("ç”¨æˆ· Alice å–œæ¬¢å–å’–å•¡", user_id="alice")

# æœç´¢
results = memory.search_memory("Alice çš„é¥®é£Ÿä¹ æƒ¯", user_id="alice")
```

---

## 5ï¸âƒ£ Task æ¨¡æ¿

| åœºæ™¯ | Task æ¨¡æ¿ |
|:---|:---|
| ç½‘é¡µæœç´¢ | `Given a web search query, retrieve relevant passages` |
| è®°å¿†æ£€ç´¢ | `Given a user query, retrieve relevant memories` |
| æ–‡æ¡£æ£€ç´¢ | `Given a question, find relevant documents` |
| ä»£ç æœç´¢ | `Given a coding query, retrieve relevant code snippets` |

---

## 6ï¸âƒ£ å…³é”®è¦ç‚¹

### âœ… å¿…é¡»è®°ä½
1. **Query åŠ  Taskï¼ŒDocument ä¸åŠ **
2. **Task æè¿°ä»»åŠ¡ç›®æ ‡**ï¼ˆæ£€ç´¢ã€åˆ†ç±»ã€èšç±»ç­‰ï¼‰
3. **æ‰¹é‡å¤„ç†æå‡æ€§èƒ½**ï¼ˆvLLMï¼‰
4. **ç›¸ä¼¼åº¦ = ä½™å¼¦ç›¸ä¼¼åº¦**ï¼ˆå‘é‡ç‚¹ç§¯ï¼‰

### ğŸ“Š æ€§èƒ½æ•°æ®
- **ç²¾åº¦æå‡**ï¼š+3-5%ï¼ˆä½¿ç”¨ Taskï¼‰
- **MTEB æ’å**ï¼š70.58ï¼ˆç¬¬ä¸€ï¼‰
- **æˆæœ¬**ï¼šÂ¥0.05/1M tokensï¼ˆ8Bï¼‰

### ğŸ”§ ä¼˜åŒ–å»ºè®®
- ä½¿ç”¨ `flash_attention_2` åŠ é€Ÿ
- æ‰¹é‡å¤„ç†ï¼ˆvLLMï¼‰
- å‘é‡ç»´åº¦å¯è°ƒï¼ˆ32-4096ï¼‰
- æ”¯æŒè‡ªå®šä¹‰ Task

---

## 7ï¸âƒ£ å®Œæ•´ç¤ºä¾‹

```python
# åœºæ™¯ï¼šæ™ºèƒ½é—®ç­”ç³»ç»Ÿ

from vllm import LLM
import torch

# 1. åˆå§‹åŒ–
model = LLM(model="Qwen/Qwen3-Embedding-8B", task="embed")

# 2. å®šä¹‰ Task
task = "Given a user question, retrieve the best answer"

# 3. å‡†å¤‡æ•°æ®
questions = [
    f"Instruct: {task}\nQuery: Python æ˜¯ä»€ä¹ˆ?",
    f"Instruct: {task}\nQuery: å¦‚ä½•å­¦ä¹  AI?"
]

answers = [
    "Python æ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€ï¼Œå¹¿æ³›ç”¨äºæ•°æ®ç§‘å­¦å’Œ AIã€‚",
    "å­¦ä¹  AI éœ€è¦æŒæ¡æ•°å­¦ã€ç¼–ç¨‹å’Œæœºå™¨å­¦ä¹ åŸºç¡€ã€‚",
    "JavaScript æ˜¯ä¸€ç§ç”¨äº Web å¼€å‘çš„è¯­è¨€ã€‚"
]

# 4. ç”Ÿæˆ embeddings
all_texts = questions + answers
outputs = model.embed(all_texts)
embeddings = torch.tensor([o.outputs.embedding for o in outputs])

# 5. è®¡ç®—åŒ¹é…åº¦
scores = embeddings[:2] @ embeddings[2:].T
print(scores)
# [[0.85, 0.12, 0.08],   # é—®é¢˜1 æœ€åŒ¹é… ç­”æ¡ˆ1
#  [0.15, 0.82, 0.09]]   # é—®é¢˜2 æœ€åŒ¹é… ç­”æ¡ˆ2

# 6. è¿”å›æœ€ä½³ç­”æ¡ˆ
for i, question in enumerate(["Python æ˜¯ä»€ä¹ˆ?", "å¦‚ä½•å­¦ä¹  AI?"]):
    best_idx = scores[i].argmax()
    print(f"Q: {question}")
    print(f"A: {answers[best_idx]}\n")
```

---

**å®Œ**

æ˜ç™½äº†ï¼é‡æ–°æ•´ç†ï¼š

---

# ğŸ“š Qwen3 Embedding å¿«é€Ÿå…¥é—¨

## 1ï¸âƒ£ ä»€ä¹ˆæ˜¯ Qwen3 Embeddingï¼Ÿ

**ä¸“é—¨çš„æ–‡æœ¬åµŒå…¥æ¨¡å‹**ï¼Œç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡ï¼Œå®ç°è¯­ä¹‰æ£€ç´¢ã€‚

| è§„æ ¼ | å‚æ•° |
|:---|:---|
| æ¨¡å‹ç³»åˆ— | 0.6B, 4B, 8B |
| å‘é‡ç»´åº¦ | 1024, 2560, 4096 |
| ä¸Šä¸‹æ–‡é•¿åº¦ | 32K tokens |
| MTEB åˆ†æ•° | 70.58ï¼ˆ8Bï¼Œç¬¬ä¸€åï¼‰ |
| æ”¯æŒè¯­è¨€ | 100+ |

---

## 2ï¸âƒ£ Task Instruction æœºåˆ¶

### æ ¸å¿ƒæ¦‚å¿µ
```
Task = å‘Šè¯‰æ¨¡å‹"è¦åšä»€ä¹ˆä»»åŠ¡"
Query = ç”¨æˆ·çš„æœç´¢/é—®é¢˜ï¼ˆéœ€è¦åŠ  Taskï¼‰
Document = è¢«æœç´¢çš„å†…å®¹ï¼ˆä¸éœ€è¦åŠ  Taskï¼‰
```

### éå¯¹ç§°æ£€ç´¢
```python
# Query ç«¯ï¼šåŠ  Task
query = "Instruct: Given a web search query, retrieve relevant passages
         Query: What is the capital of China?"

# Document ç«¯ï¼šä¸åŠ  Task
document = "The capital of China is Beijing."
```

**åŸå› **ï¼š
- Query çŸ­å°æ¨¡ç³Šï¼Œéœ€è¦ä¸Šä¸‹æ–‡
- Document å†…å®¹å®Œæ•´ï¼Œä¸éœ€è¦é¢å¤–è¯´æ˜

**æ•ˆæœ**ï¼šç²¾åº¦æå‡ 3-5%

---

## 3ï¸âƒ£ ä¸¤ç§ä½¿ç”¨æ–¹å¼

### æ–¹å¼1ï¼šsentence-transformersï¼ˆæ¨èï¼‰

```python
from sentence_transformers import SentenceTransformer

# åŠ è½½æ¨¡å‹
model = SentenceTransformer("Qwen/Qwen3-Embedding-8B")

# Query åŠ  prompt
query_embeddings = model.encode(
    ["What is the capital of China?"],
    prompt_name="query"  # ä½¿ç”¨å†…ç½® query prompt
)

# Document ä¸åŠ  prompt
doc_embeddings = model.encode([
    "The capital of China is Beijing."
])

# è®¡ç®—ç›¸ä¼¼åº¦
similarity = model.similarity(query_embeddings, doc_embeddings)
# è¾“å‡º: [[0.7493]]
```

### æ–¹å¼2ï¼švLLMï¼ˆé«˜æ€§èƒ½ï¼‰

```python
from vllm import LLM
import torch

# åŠ è½½æ¨¡å‹
model = LLM(model="Qwen/Qwen3-Embedding-8B", task="embed")

# æ„é€  Task Instruction
def get_instruct(task, query):
    return f'Instruct: {task}\nQuery:{query}'

task = 'Given a web search query, retrieve relevant passages'

# Query åŠ  task
queries = [
    get_instruct(task, 'What is the capital of China?'),
    get_instruct(task, 'Explain gravity')
]

# Document ä¸åŠ  task
documents = [
    "The capital of China is Beijing.",
    "Gravity is a force that attracts..."
]

# æ‰¹é‡ç¼–ç 
outputs = model.embed(queries + documents)
embeddings = torch.tensor([o.outputs.embedding for o in outputs])

# è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
scores = embeddings[:2] @ embeddings[2:].T
# è¾“å‡º: [[0.748, 0.076], [0.089, 0.630]]
```

---

## 4ï¸âƒ£ å®æˆ˜ï¼šè®°å¿†ç®¡ç†ç³»ç»Ÿ

```python
class MemorySystem:
    def __init__(self):
        self.model = LLM(model="Qwen/Qwen3-Embedding-8B", task="embed")
    
    def store_memory(self, text: str, user_id: str):
        """å­˜å‚¨è®°å¿† - ä¸åŠ  task"""
        embedding = self.model.embed([text])[0].outputs.embedding
        vector_db.insert(
            embedding=embedding,
            text=text,
            user_id=user_id
        )
    
    def search_memory(self, query: str, user_id: str):
        """æœç´¢è®°å¿† - åŠ  task"""
        task = "Given a user query, retrieve relevant memories"
        query_with_task = f"Instruct: {task}\nQuery: {query}"
        
        query_embedding = self.model.embed([query_with_task])[0].outputs.embedding
        
        results = vector_db.search(
            embedding=query_embedding,
            filter={"user_id": user_id},
            top_k=5
        )
        return results

# ä½¿ç”¨
memory = MemorySystem()

# å­˜å‚¨
memory.store_memory("ç”¨æˆ· Alice å–œæ¬¢å–å’–å•¡", user_id="alice")

# æœç´¢
results = memory.search_memory("Alice çš„é¥®é£Ÿä¹ æƒ¯", user_id="alice")
```

---

## 5ï¸âƒ£ Task æ¨¡æ¿

| åœºæ™¯ | Task æ¨¡æ¿ |
|:---|:---|
| ç½‘é¡µæœç´¢ | `Given a web search query, retrieve relevant passages` |
| è®°å¿†æ£€ç´¢ | `Given a user query, retrieve relevant memories` |
| æ–‡æ¡£æ£€ç´¢ | `Given a question, find relevant documents` |
| ä»£ç æœç´¢ | `Given a coding query, retrieve relevant code snippets` |

---

## 6ï¸âƒ£ å…³é”®è¦ç‚¹

### âœ… å¿…é¡»è®°ä½
1. **Query åŠ  Taskï¼ŒDocument ä¸åŠ **
2. **Task æè¿°ä»»åŠ¡ç›®æ ‡**ï¼ˆæ£€ç´¢ã€åˆ†ç±»ã€èšç±»ç­‰ï¼‰
3. **æ‰¹é‡å¤„ç†æå‡æ€§èƒ½**ï¼ˆvLLMï¼‰
4. **ç›¸ä¼¼åº¦ = ä½™å¼¦ç›¸ä¼¼åº¦**ï¼ˆå‘é‡ç‚¹ç§¯ï¼‰

### ğŸ“Š æ€§èƒ½æ•°æ®
- **ç²¾åº¦æå‡**ï¼š+3-5%ï¼ˆä½¿ç”¨ Taskï¼‰
- **MTEB æ’å**ï¼š70.58ï¼ˆç¬¬ä¸€ï¼‰
- **æˆæœ¬**ï¼šÂ¥0.05/1M tokensï¼ˆ8Bï¼‰

### ğŸ”§ ä¼˜åŒ–å»ºè®®
- ä½¿ç”¨ `flash_attention_2` åŠ é€Ÿ
- æ‰¹é‡å¤„ç†ï¼ˆvLLMï¼‰
- å‘é‡ç»´åº¦å¯è°ƒï¼ˆ32-4096ï¼‰
- æ”¯æŒè‡ªå®šä¹‰ Task

---

## 7ï¸âƒ£ å®Œæ•´ç¤ºä¾‹

```python
# åœºæ™¯ï¼šæ™ºèƒ½é—®ç­”ç³»ç»Ÿ

from vllm import LLM
import torch

# 1. åˆå§‹åŒ–
model = LLM(model="Qwen/Qwen3-Embedding-8B", task="embed")

# 2. å®šä¹‰ Task
task = "Given a user question, retrieve the best answer"

# 3. å‡†å¤‡æ•°æ®
questions = [
    f"Instruct: {task}\nQuery: Python æ˜¯ä»€ä¹ˆ?",
    f"Instruct: {task}\nQuery: å¦‚ä½•å­¦ä¹  AI?"
]

answers = [
    "Python æ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€ï¼Œå¹¿æ³›ç”¨äºæ•°æ®ç§‘å­¦å’Œ AIã€‚",
    "å­¦ä¹  AI éœ€è¦æŒæ¡æ•°å­¦ã€ç¼–ç¨‹å’Œæœºå™¨å­¦ä¹ åŸºç¡€ã€‚",
    "JavaScript æ˜¯ä¸€ç§ç”¨äº Web å¼€å‘çš„è¯­è¨€ã€‚"
]

# 4. ç”Ÿæˆ embeddings
all_texts = questions + answers
outputs = model.embed(all_texts)
embeddings = torch.tensor([o.outputs.embedding for o in outputs])

# 5. è®¡ç®—åŒ¹é…åº¦
scores = embeddings[:2] @ embeddings[2:].T
print(scores)
# [[0.85, 0.12, 0.08],   # é—®é¢˜1 æœ€åŒ¹é… ç­”æ¡ˆ1
#  [0.15, 0.82, 0.09]]   # é—®é¢˜2 æœ€åŒ¹é… ç­”æ¡ˆ2

# 6. è¿”å›æœ€ä½³ç­”æ¡ˆ
for i, question in enumerate(["Python æ˜¯ä»€ä¹ˆ?", "å¦‚ä½•å­¦ä¹  AI?"]):
    best_idx = scores[i].argmax()
    print(f"Q: {question}")
    print(f"A: {answers[best_idx]}\n")
```

