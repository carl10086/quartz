# vLLM æ–‡æ¡£

## æ¦‚è¿°

vLLM æ˜¯ä¸€ä¸ªå¿«é€Ÿä¸”æ˜“äºä½¿ç”¨çš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†å’ŒæœåŠ¡åº“ã€‚

æœ€åˆç”±åŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡çš„ Sky Computing Lab å¼€å‘ï¼ŒvLLM ç°å·²å‘å±•æˆä¸ºä¸€ä¸ªç¤¾åŒºé©±åŠ¨çš„é¡¹ç›®ï¼Œæ±‡é›†äº†æ¥è‡ªå­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œçš„è´¡çŒ®ã€‚

## åŸºç¡€æ¦‚å¿µè¯¦è§£

### æ³¨æ„åŠ›æœºåˆ¶ä¸ KV Cache

#### ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„ Qã€Kã€Vï¼Ÿ

åœ¨ Transformer çš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œæ¯ä¸ªè¾“å…¥éƒ½ä¼šè¢«è½¬æ¢æˆä¸‰ä¸ªå‘é‡ï¼š

- **Q (Query)**ï¼šæŸ¥è¯¢å‘é‡ï¼Œè¡¨ç¤º"æˆ‘æƒ³è¦ä»€ä¹ˆä¿¡æ¯"
- **K (Key)**ï¼šé”®å‘é‡ï¼Œè¡¨ç¤º"æˆ‘æœ‰ä»€ä¹ˆä¿¡æ¯"  
- **V (Value)**ï¼šå€¼å‘é‡ï¼Œè¡¨ç¤º"å…·ä½“çš„ä¿¡æ¯å†…å®¹"

```mermaid
graph LR
    A[è¾“å…¥ Token] --> B[çº¿æ€§å˜æ¢]
    B --> C[Q æŸ¥è¯¢]
    B --> D[K é”®]
    B --> E[V å€¼]
    
    C --> F[æ³¨æ„åŠ›è®¡ç®—]
    D --> F
    E --> G[åŠ æƒæ±‚å’Œ]
    F --> G
    G --> H[è¾“å‡º]
```

#### æ³¨æ„åŠ›è®¡ç®—è¿‡ç¨‹

æ³¨æ„åŠ›çš„è®¡ç®—å…¬å¼ï¼š
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

**æ­¥éª¤è§£æ**ï¼š
1. **è®¡ç®—ç›¸ä¼¼åº¦**ï¼š$QK^T$ - Q ä¸æ‰€æœ‰ K çš„ç‚¹ç§¯ï¼Œå¾—åˆ°æ³¨æ„åŠ›åˆ†æ•°
2. **å½’ä¸€åŒ–**ï¼šé™¤ä»¥ $\sqrt{d_k}$ é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±
3. **æ¦‚ç‡åŒ–**ï¼šsoftmax å°†åˆ†æ•°è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
4. **åŠ æƒæ±‚å’Œ**ï¼šç”¨æ¦‚ç‡æƒé‡å¯¹ V è¿›è¡ŒåŠ æƒå¹³å‡

#### ä»€ä¹ˆæ˜¯ KV Cacheï¼Ÿ

**KV Cache å°±æ˜¯ç¼“å­˜çš„ Kï¼ˆé”®ï¼‰å’Œ Vï¼ˆå€¼ï¼‰å‘é‡**

åœ¨æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼š

```mermaid
sequenceDiagram
    participant U as ç”¨æˆ·è¾“å…¥
    participant M as æ¨¡å‹
    participant C as KV Cache
    participant O as è¾“å‡º
    
    Note over U,O: ç¬¬ä¸€ä¸ª token ç”Ÿæˆ
    U->>M: "ä»Šå¤©å¤©æ°”"
    M->>M: è®¡ç®—æ‰€æœ‰ token çš„ K, V
    M->>C: å­˜å‚¨ K, V åˆ°ç¼“å­˜
    M->>O: ç”Ÿæˆ "å¾ˆ"
    
    Note over U,O: ç¬¬äºŒä¸ª token ç”Ÿæˆ
    M->>M: åªè®¡ç®—æ–° token "å¾ˆ" çš„ K, V
    M->>C: ä»ç¼“å­˜è¯»å–å†å² K, V
    M->>C: æ·»åŠ æ–°çš„ K, V åˆ°ç¼“å­˜
    M->>O: ç”Ÿæˆ "å¥½"
    
    Note over U,O: ç»§ç»­ç”Ÿæˆ...
```

#### ä¸ºä»€ä¹ˆéœ€è¦ KV Cacheï¼Ÿ

**æ²¡æœ‰ KV Cache çš„é—®é¢˜**ï¼š
```python
# ä¼ªä»£ç ç¤ºä¾‹
è¾“å…¥åºåˆ— = ["ä»Šå¤©", "å¤©æ°”"]
ç”Ÿæˆç¬¬1ä¸ªè¯ "å¾ˆ":
  - éœ€è¦è®¡ç®— ["ä»Šå¤©", "å¤©æ°”"] çš„ K, V
  
ç”Ÿæˆç¬¬2ä¸ªè¯ "å¥½":  
  - éœ€è¦é‡æ–°è®¡ç®— ["ä»Šå¤©", "å¤©æ°”", "å¾ˆ"] çš„ K, V  # é‡å¤è®¡ç®—ï¼
  
ç”Ÿæˆç¬¬3ä¸ªè¯:
  - éœ€è¦é‡æ–°è®¡ç®— ["ä»Šå¤©", "å¤©æ°”", "å¾ˆ", "å¥½"] çš„ K, V  # åˆæ˜¯é‡å¤è®¡ç®—ï¼
```

**ä½¿ç”¨ KV Cache çš„ä¼˜åŒ–**ï¼š
```python
# ä¼ªä»£ç ç¤ºä¾‹
è¾“å…¥åºåˆ— = ["ä»Šå¤©", "å¤©æ°”"]
ç”Ÿæˆç¬¬1ä¸ªè¯ "å¾ˆ":
  - è®¡ç®— ["ä»Šå¤©", "å¤©æ°”"] çš„ K, V
  - å­˜å‚¨åˆ° KV Cache

ç”Ÿæˆç¬¬2ä¸ªè¯ "å¥½":
  - ä» KV Cache è¯»å– ["ä»Šå¤©", "å¤©æ°”"] çš„ K, V  # æ— éœ€é‡æ–°è®¡ç®—ï¼
  - åªè®¡ç®—æ–°è¯ "å¾ˆ" çš„ K, V
  - æ›´æ–° KV Cache

ç”Ÿæˆç¬¬3ä¸ªè¯:
  - ä» KV Cache è¯»å–å†å²æ‰€æœ‰ K, V  # æ— éœ€é‡æ–°è®¡ç®—ï¼
  - åªè®¡ç®—æ–°è¯ "å¥½" çš„ K, V
```

#### KV Cache çš„å†…å­˜é—®é¢˜

**ä¼ ç»Ÿ KV Cache çš„å†…å­˜åˆ†é…**ï¼š

```mermaid
graph TB
    subgraph "ä¼ ç»Ÿå†…å­˜åˆ†é…"
        A[è¯·æ±‚1: é¢„åˆ†é…2048 tokens] --> A1[å®é™…ä½¿ç”¨: 100 tokens]
        A1 --> A2[æµªè´¹: 1948 tokens å†…å­˜]
        
        B[è¯·æ±‚2: é¢„åˆ†é…2048 tokens] --> B1[å®é™…ä½¿ç”¨: 500 tokens]  
        B1 --> B2[æµªè´¹: 1548 tokens å†…å­˜]
        
        C[è¯·æ±‚3: é¢„åˆ†é…2048 tokens] --> C1[å®é™…ä½¿ç”¨: 50 tokens]
        C1 --> C2[æµªè´¹: 1998 tokens å†…å­˜]
    end
    
    subgraph "é—®é¢˜"
        D[å¤§é‡å†…å­˜æµªè´¹]
        E[æ— æ³•å¤„ç†æ›´å¤šè¯·æ±‚]
        F[GPU å†…å­˜åˆ©ç”¨ç‡ä½]
    end
    
    A2 --> D
    B2 --> D  
    C2 --> D
    D --> E
    D --> F
```

### ä¸ºä»€ä¹ˆ Kã€V å¯ä»¥ç¼“å­˜ï¼ŒQ ä¸èƒ½ç¼“å­˜ï¼Ÿ

#### Kã€V çš„ç‰¹æ€§

**K å’Œ V æ˜¯æ ¹æ®ç¡®å®šçš„è¾“å…¥è®¡ç®—çš„**ï¼š
- **Kï¼ˆé”®ï¼‰**ï¼šåŸºäº `token_embedding + position_encoding` è®¡ç®—ï¼Œè¡¨ç¤º"æˆ‘åœ¨ç‰¹å®šä½ç½®æä¾›ä»€ä¹ˆä¿¡æ¯"
- **Vï¼ˆå€¼ï¼‰**ï¼šåŒæ ·åŸºäºç¡®å®šçš„è¾“å…¥è®¡ç®—ï¼Œè¡¨ç¤º"æˆ‘åœ¨ç‰¹å®šä½ç½®çš„å…·ä½“å†…å®¹"
- ä¸€æ—¦æŸä¸ªä½ç½®çš„ token ç¡®å®šï¼Œå…¶ K å’Œ V å°±å›ºå®šä¸å˜

```python
# K, V çš„è®¡ç®—æ–¹å¼
input_0 = embedding("ä»Šå¤©") + position_encoding[0]
Kâ‚€ = Linear_K(input_0)  # "ä½ç½®0çš„'ä»Šå¤©'æä¾›ä»€ä¹ˆä¿¡æ¯ï¼Ÿ" - å›ºå®šä¸å˜
Vâ‚€ = Linear_V(input_0)  # "ä½ç½®0çš„'ä»Šå¤©'çš„å…·ä½“å†…å®¹" - å›ºå®šä¸å˜

input_1 = embedding("å¤©æ°”") + position_encoding[1]  
Kâ‚ = Linear_K(input_1)  # "ä½ç½®1çš„'å¤©æ°”'æä¾›ä»€ä¹ˆä¿¡æ¯ï¼Ÿ" - å›ºå®šä¸å˜
Vâ‚ = Linear_V(input_1)  # "ä½ç½®1çš„'å¤©æ°”'çš„å…·ä½“å†…å®¹" - å›ºå®šä¸å˜
```

#### Q çš„ç‰¹æ€§

**Q æ˜¯åŸºäºå½“å‰ decoder_state è®¡ç®—çš„**ï¼š
- **Qï¼ˆæŸ¥è¯¢ï¼‰**ï¼šåŸºäºå½“å‰çš„ `decoder_state_at_position` è®¡ç®—ï¼Œè¡¨ç¤º"æˆ‘åœ¨å½“å‰ä½ç½®æƒ³è¦ä»€ä¹ˆä¿¡æ¯"
- æ¯ä¸ªæ–°ä½ç½®çš„ Q éƒ½ä»£è¡¨å…¨æ–°çš„æŸ¥è¯¢éœ€æ±‚
- Q å¿…é¡»åŸºäºå½“å‰çš„ç”Ÿæˆä¸Šä¸‹æ–‡é‡æ–°è®¡ç®—

```python
# Q çš„è®¡ç®—æ–¹å¼
Qâ‚ = Linear_Q(decoder_state_at_position_1)  # "åŸºäºå½“å‰çŠ¶æ€ï¼Œæˆ‘æƒ³è¡¨è¾¾ä»€ä¹ˆï¼Ÿ"
Qâ‚‚ = Linear_Q(decoder_state_at_position_2)  # "åŸºäºæ–°çš„çŠ¶æ€ï¼Œæˆ‘æƒ³è¡¨è¾¾ä»€ä¹ˆï¼Ÿ"
Qâ‚ƒ = Linear_Q(decoder_state_at_position_3)  # "åŸºäºæ›´æ–°çš„çŠ¶æ€ï¼Œæˆ‘æƒ³è¡¨è¾¾ä»€ä¹ˆï¼Ÿ"
```

#### è¯¦ç»†çš„ç”Ÿæˆè¿‡ç¨‹å¯¹æ¯”

è®©æˆ‘ä»¬ç”¨ç”Ÿæˆå¥å­"ä»Šå¤©å¤©æ°”å¾ˆå¥½"çš„ä¾‹å­æ¥è¯¦ç»†çœ‹çœ‹ï¼š

**ç¬¬1æ­¥ï¼šç”Ÿæˆ "å¤©æ°”"**
```python
# çŠ¶æ€ï¼šå·²æœ‰ ["ä»Šå¤©"]ï¼Œè¦ç”Ÿæˆä¸‹ä¸€ä¸ªtoken

# è¾“å…¥å‡†å¤‡
input_0 = embedding("ä»Šå¤©") + position_encoding[0]

# è®¡ç®— Q,K,V
Qâ‚€ = Linear_Q(decoder_state_0)  # "åœ¨ä½ç½®0ï¼ŒåŸºäº'ä»Šå¤©'ï¼Œæˆ‘æƒ³è¡¨è¾¾ä»€ä¹ˆï¼Ÿ"
Kâ‚€ = Linear_K(input_0)          # "ä½ç½®0çš„'ä»Šå¤©'æä¾›ä»€ä¹ˆä¿¡æ¯ï¼Ÿ"  
Vâ‚€ = Linear_V(input_0)          # "ä½ç½®0çš„'ä»Šå¤©'çš„å…·ä½“å†…å®¹"

# æ³¨æ„åŠ›è®¡ç®—ï¼ˆè‡ªæ³¨æ„åŠ›ï¼‰
attention_0 = softmax(Qâ‚€ @ Kâ‚€áµ€) @ Vâ‚€

# é€šè¿‡åç»­å±‚å¤„ç†ï¼Œæœ€ç»ˆè¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ
# é‡‡æ ·å¾—åˆ°ï¼šä¸‹ä¸€ä¸ªtoken = "å¤©æ°”"

# ç¼“å­˜ç­–ç•¥
KV_Cache = {position_0: (Kâ‚€, Vâ‚€)}  # ç¼“å­˜ä½ç½®0çš„K,V
```

**ç¬¬2æ­¥ï¼šç”Ÿæˆ "å¾ˆ"**
```python
# çŠ¶æ€ï¼šå·²æœ‰ ["ä»Šå¤©", "å¤©æ°”"]ï¼Œè¦ç”Ÿæˆä¸‹ä¸€ä¸ªtoken

# æ–°è¾“å…¥å‡†å¤‡  
input_1 = embedding("å¤©æ°”") + position_encoding[1]

# è®¡ç®—æ–°çš„ K,Vï¼ˆä½ç½®1ï¼‰
Kâ‚ = Linear_K(input_1)  # "ä½ç½®1çš„'å¤©æ°”'æä¾›ä»€ä¹ˆä¿¡æ¯ï¼Ÿ"
Vâ‚ = Linear_V(input_1)  # "ä½ç½®1çš„'å¤©æ°”'çš„å…·ä½“å†…å®¹"

# è®¡ç®—æ–°çš„ Qï¼ˆå½“å‰ç”Ÿæˆä½ç½®ï¼‰
Qâ‚‚ = Linear_Q(decoder_state_at_position_2)  # "åœ¨ä½ç½®2ï¼ŒåŸºäºå‰é¢çš„å†…å®¹ï¼Œæˆ‘æƒ³è¡¨è¾¾ä»€ä¹ˆï¼Ÿ"

# ä»ç¼“å­˜è¯»å–å†å² K,V
K_history = [Kâ‚€, Kâ‚]  # ä»ç¼“å­˜è¯»å– + æ–°è®¡ç®—çš„
V_history = [Vâ‚€, Vâ‚]  # ä»ç¼“å­˜è¯»å– + æ–°è®¡ç®—çš„

# æ³¨æ„åŠ›è®¡ç®—ï¼ˆå› æœæ³¨æ„åŠ›ï¼‰
attention_scores = Qâ‚‚ @ [Kâ‚€, Kâ‚]áµ€  # Qâ‚‚ä¸æ‰€æœ‰å†å²Kè®¡ç®—ç›¸ä¼¼åº¦
attention_weights = softmax(attention_scores)
attention_output = attention_weights @ [Vâ‚€, Vâ‚]

# è¾“å‡ºä¸‹ä¸€ä¸ªtoken = "å¾ˆ"

# æ›´æ–°ç¼“å­˜
KV_Cache = {
    position_0: (Kâ‚€, Vâ‚€),  # ä¿æŒä¸å˜
    position_1: (Kâ‚, Vâ‚)   # æ–°å¢
}
```

**ç¬¬3æ­¥ï¼šç”Ÿæˆ "å¥½"**
```python
# çŠ¶æ€ï¼šå·²æœ‰ ["ä»Šå¤©", "å¤©æ°”", "å¾ˆ"]ï¼Œè¦ç”Ÿæˆä¸‹ä¸€ä¸ªtoken

# æ–°è¾“å…¥å‡†å¤‡
input_2 = embedding("å¾ˆ") + position_encoding[2]

# è®¡ç®—æ–°çš„ K,Vï¼ˆä½ç½®2ï¼‰
Kâ‚‚ = Linear_K(input_2)  # "ä½ç½®2çš„'å¾ˆ'æä¾›ä»€ä¹ˆä¿¡æ¯ï¼Ÿ"
Vâ‚‚ = Linear_V(input_2)  # "ä½ç½®2çš„'å¾ˆ'çš„å…·ä½“å†…å®¹"

# è®¡ç®—æ–°çš„ Qï¼ˆä½ç½®3çš„æŸ¥è¯¢ï¼‰
Qâ‚ƒ = Linear_Q(decoder_state_at_position_3)  # "åœ¨ä½ç½®3ï¼ŒåŸºäº'ä»Šå¤©å¤©æ°”å¾ˆ'ï¼Œæˆ‘æƒ³è¡¨è¾¾ä»€ä¹ˆï¼Ÿ"

# ä»ç¼“å­˜è¯»å–æ‰€æœ‰å†å² K,V
K_history = [Kâ‚€, Kâ‚, Kâ‚‚]  # ç¼“å­˜ + ç¼“å­˜ + æ–°è®¡ç®—
V_history = [Vâ‚€, Vâ‚, Vâ‚‚]  # ç¼“å­˜ + ç¼“å­˜ + æ–°è®¡ç®—

# æ³¨æ„åŠ›è®¡ç®—
attention_scores = Qâ‚ƒ @ [Kâ‚€, Kâ‚, Kâ‚‚]áµ€
attention_weights = softmax(attention_scores)  
attention_output = attention_weights @ [Vâ‚€, Vâ‚, Vâ‚‚]

# è¾“å‡ºä¸‹ä¸€ä¸ªtoken = "å¥½"

# æ›´æ–°ç¼“å­˜
KV_Cache = {
    position_0: (Kâ‚€, Vâ‚€),  # ä¿æŒä¸å˜
    position_1: (Kâ‚, Vâ‚),  # ä¿æŒä¸å˜  
    position_2: (Kâ‚‚, Vâ‚‚)   # æ–°å¢
}
```

#### å…³é”®ç†è§£ç‚¹

```mermaid
sequenceDiagram
    participant D as DecoderçŠ¶æ€
    participant C as KV Cache
    participant A as æ³¨æ„åŠ›å±‚
    participant O as è¾“å‡ºå±‚
    
    Note over D,O: ç”Ÿæˆ "å¤©æ°”" (ä½ç½®1)
    D->>D: åŸºäº"ä»Šå¤©"çš„decoderçŠ¶æ€
    D->>A: Qâ‚ = "åŸºäº'ä»Šå¤©'ï¼Œä½ç½®1æƒ³è¯´ä»€ä¹ˆï¼Ÿ"
    D->>C: å­˜å‚¨ Kâ‚€,Vâ‚€ ("ä»Šå¤©"çš„é”®å€¼)
    A->>A: Attention(Qâ‚, [Kâ‚€], [Vâ‚€])
    A->>O: è¾“å‡º "å¤©æ°”"
    
    Note over D,O: ç”Ÿæˆ "å¾ˆ" (ä½ç½®2)  
    D->>D: åŸºäº"ä»Šå¤©å¤©æ°”"çš„decoderçŠ¶æ€
    D->>A: Qâ‚‚ = "åŸºäº'ä»Šå¤©å¤©æ°”'ï¼Œä½ç½®2æƒ³è¯´ä»€ä¹ˆï¼Ÿ"
    D->>C: å­˜å‚¨ Kâ‚,Vâ‚ ("å¤©æ°”"çš„é”®å€¼)
    C->>A: è¯»å– [Kâ‚€,Kâ‚], [Vâ‚€,Vâ‚]
    A->>A: Attention(Qâ‚‚, [Kâ‚€,Kâ‚], [Vâ‚€,Vâ‚])
    A->>O: è¾“å‡º "å¾ˆ"
    
    Note over D,O: ç”Ÿæˆ "å¥½" (ä½ç½®3)
    D->>D: åŸºäº"ä»Šå¤©å¤©æ°”å¾ˆ"çš„decoderçŠ¶æ€  
    D->>A: Qâ‚ƒ = "åŸºäº'ä»Šå¤©å¤©æ°”å¾ˆ'ï¼Œä½ç½®3æƒ³è¯´ä»€ä¹ˆï¼Ÿ"
    D->>C: å­˜å‚¨ Kâ‚‚,Vâ‚‚ ("å¾ˆ"çš„é”®å€¼)
    C->>A: è¯»å– [Kâ‚€,Kâ‚,Kâ‚‚], [Vâ‚€,Vâ‚,Vâ‚‚]
    A->>A: Attention(Qâ‚ƒ, [Kâ‚€,Kâ‚,Kâ‚‚], [Vâ‚€,Vâ‚,Vâ‚‚])
    A->>O: è¾“å‡º "å¥½"
```

#### ä¸ºä»€ä¹ˆ Q ä¸èƒ½ç¼“å­˜çš„æ ¹æœ¬åŸå› 

```python
# æ¯ä¸€æ­¥çš„Qéƒ½ä»£è¡¨ä¸åŒçš„"æŸ¥è¯¢æ„å›¾"ï¼š

Qâ‚ = "æˆ‘åœ¨ä½ç½®1ï¼ŒåŸºäº'ä»Šå¤©'ï¼Œæˆ‘æƒ³è¯´ä»€ä¹ˆï¼Ÿ"           # ç­”æ¡ˆï¼šå¤©æ°”
Qâ‚‚ = "æˆ‘åœ¨ä½ç½®2ï¼ŒåŸºäº'ä»Šå¤©å¤©æ°”'ï¼Œæˆ‘æƒ³è¯´ä»€ä¹ˆï¼Ÿ"       # ç­”æ¡ˆï¼šå¾ˆ  
Qâ‚ƒ = "æˆ‘åœ¨ä½ç½®3ï¼ŒåŸºäº'ä»Šå¤©å¤©æ°”å¾ˆ'ï¼Œæˆ‘æƒ³è¯´ä»€ä¹ˆï¼Ÿ"     # ç­”æ¡ˆï¼šå¥½

# è€ŒK,Vä»£è¡¨"å·²ç¡®å®šä¿¡æ¯çš„å›ºå®šè¡¨ç¤º"ï¼š
Kâ‚€,Vâ‚€ = "ä½ç½®0çš„'ä»Šå¤©'æä¾›çš„ä¿¡æ¯"  # å›ºå®šä¸å˜
Kâ‚,Vâ‚ = "ä½ç½®1çš„'å¤©æ°”'æä¾›çš„ä¿¡æ¯"  # å›ºå®šä¸å˜
Kâ‚‚,Vâ‚‚ = "ä½ç½®2çš„'å¾ˆ'æä¾›çš„ä¿¡æ¯"    # å›ºå®šä¸å˜
```

#### å¦‚æœå¼ºè¡Œç¼“å­˜ Q ä¼šæ€æ ·ï¼Ÿ

```python
# é”™è¯¯çš„åšæ³•
å†å²Qç¼“å­˜: [Qâ‚, Qâ‚‚, Qâ‚ƒ]  # è¿™äº›æ˜¯å†å²ä½ç½®çš„æŸ¥è¯¢éœ€æ±‚

# é—®é¢˜ï¼š
Qâ‚ = "åœ¨ä½ç½®1ï¼ŒåŸºäº'ä»Šå¤©'ï¼Œæˆ‘æƒ³è¦ä»€ä¹ˆï¼Ÿ"     # å·²ç»è¿‡æ—¶
Qâ‚‚ = "åœ¨ä½ç½®2ï¼ŒåŸºäº'ä»Šå¤©å¤©æ°”'ï¼Œæˆ‘æƒ³è¦ä»€ä¹ˆï¼Ÿ"  # å·²ç»è¿‡æ—¶  
Qâ‚ƒ = "åœ¨ä½ç½®3ï¼ŒåŸºäº'ä»Šå¤©å¤©æ°”å¾ˆ'ï¼Œæˆ‘æƒ³è¦ä»€ä¹ˆï¼Ÿ" # å·²ç»è¿‡æ—¶

# å½“å‰éœ€è¦çš„ï¼š
Qâ‚„ = "åœ¨ä½ç½®4ï¼ŒåŸºäº'ä»Šå¤©å¤©æ°”å¾ˆå¥½'ï¼Œæˆ‘æƒ³è¦ä»€ä¹ˆï¼Ÿ" # å…¨æ–°çš„æŸ¥è¯¢éœ€æ±‚
```

### Decoder State çš„æ·±å…¥ç†è§£

#### ä»€ä¹ˆæ˜¯ Decoder Stateï¼Ÿ

`decoder_state_at_position` æ˜¯**ç»è¿‡å¤šå±‚ Transformer å¤„ç†åçš„éšè—çŠ¶æ€**ï¼Œå®ƒåŒ…å«äº†åˆ°å½“å‰ä½ç½®ä¸ºæ­¢çš„æ‰€æœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

#### Decoder State çš„è®¡ç®—æµç¨‹

```mermaid
graph TB
    subgraph "è¾“å…¥å±‚"
        A[Token Embeddings] --> B[+ Position Encodings]
        B --> C[Input Representations]
    end
    
    subgraph "Transformer å±‚ 1"
        C --> D[Multi-Head Attention 1]
        D --> E[Add & Norm 1]
        E --> F[Feed Forward 1]
        F --> G[Add & Norm 1]
    end
    
    subgraph "Transformer å±‚ 2"
        G --> H[Multi-Head Attention 2]
        H --> I[Add & Norm 2]
        I --> J[Feed Forward 2]
        J --> K[Add & Norm 2]
    end
    
    subgraph "..."
        K --> L[... æ›´å¤šå±‚]
    end
    
    subgraph "æœ€ç»ˆå±‚"
        L --> M[Multi-Head Attention N]
        M --> N[Add & Norm N]
        N --> O[Feed Forward N]
        O --> P[Decoder State]
    end
    
    P --> Q[Linear_Q]
    P --> R[Linear_K]
    P --> S[Linear_V]
```

#### å…·ä½“çš„è®¡ç®—è¿‡ç¨‹

è®©æˆ‘ä»¬ç”¨ä¾‹å­æ¥çœ‹æ¯ä¸€æ­¥ï¼š

```python
# å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ª 2 å±‚çš„ Transformer Decoder
# å½“å‰çŠ¶æ€ï¼š["ä»Šå¤©", "å¤©æ°”"]ï¼Œè¦ç”Ÿæˆä½ç½® 2

# === è¾“å…¥å‡†å¤‡ ===
tokens = ["ä»Šå¤©", "å¤©æ°”"]
positions = [0, 1]

# è¾“å…¥è¡¨ç¤º
input_0 = embedding("ä»Šå¤©") + position_encoding[0]
input_1 = embedding("å¤©æ°”") + position_encoding[1]
inputs = [input_0, input_1]  # shape: [seq_len=2, hidden_dim]

# === ç¬¬ 1 å±‚ Transformer ===
# å¤šå¤´æ³¨æ„åŠ›
Q1 = Linear_Q1(inputs)  # [2, hidden_dim]
K1 = Linear_K1(inputs)  # [2, hidden_dim]  
V1 = Linear_V1(inputs)  # [2, hidden_dim]

# å› æœæ©ç æ³¨æ„åŠ›ï¼ˆåªèƒ½çœ‹åˆ°ä¹‹å‰çš„tokenï¼‰
attention_mask = [[1, 0],    # ä½ç½®0åªèƒ½çœ‹åˆ°è‡ªå·±
                  [1, 1]]    # ä½ç½®1å¯ä»¥çœ‹åˆ°ä½ç½®0å’Œè‡ªå·±

attention_output1 = CausalAttention(Q1, K1, V1, mask=attention_mask)
# Add & Norm
layer1_output = LayerNorm(inputs + attention_output1)

# Feed Forward
ff_output1 = FeedForward(layer1_output)
# Add & Norm  
layer1_final = LayerNorm(layer1_output + ff_output1)

# === ç¬¬ 2 å±‚ Transformer ===
# å¤šå¤´æ³¨æ„åŠ›
Q2 = Linear_Q2(layer1_final)
K2 = Linear_K2(layer1_final)
V2 = Linear_V2(layer1_final)

attention_output2 = CausalAttention(Q2, K2, V2, mask=attention_mask)
layer2_output = LayerNorm(layer1_final + attention_output2)

ff_output2 = FeedForward(layer2_output)
layer2_final = LayerNorm(layer2_output + ff_output2)

# === æœ€ç»ˆçš„ Decoder State ===
decoder_states = layer2_final  # shape: [2, hidden_dim]
# decoder_states[0] = ä½ç½®0ç»è¿‡æ‰€æœ‰å±‚å¤„ç†åçš„çŠ¶æ€
# decoder_states[1] = ä½ç½®1ç»è¿‡æ‰€æœ‰å±‚å¤„ç†åçš„çŠ¶æ€
```

#### ç”Ÿæˆä¸‹ä¸€ä¸ª Token æ—¶çš„çŠ¶æ€

```python
# ç°åœ¨è¦ç”Ÿæˆä½ç½® 2 çš„ token
# å…³é”®ï¼šä½ç½® 2 è¿˜æ²¡æœ‰ç¡®å®šçš„ tokenï¼Œä½†æˆ‘ä»¬éœ€è¦å®ƒçš„ decoder state

# æ–¹æ³•1ï¼šä½¿ç”¨ç‰¹æ®Šçš„"ç”Ÿæˆä½ç½®"è¡¨ç¤º
generation_input = special_generation_embedding + position_encoding[2]

# æ–¹æ³•2ï¼šä½¿ç”¨å‰ä¸€ä¸ªä½ç½®çš„çŠ¶æ€ä½œä¸ºèµ·ç‚¹
# å®é™…ä¸Šï¼Œç°ä»£å®ç°é€šå¸¸ä½¿ç”¨ä½ç½®1çš„æœ€ç»ˆçŠ¶æ€ä½œä¸ºä½ç½®2çš„åˆå§‹æŸ¥è¯¢

# è®¡ç®—ä½ç½® 2 çš„æŸ¥è¯¢
decoder_state_at_position_2 = decoder_states[1]  # ä½¿ç”¨ä½ç½®1çš„æœ€ç»ˆçŠ¶æ€
Q_for_position_2 = Linear_Q_final(decoder_state_at_position_2)
```

#### æ›´å‡†ç¡®çš„ç†è§£

å®é™…ä¸Šï¼Œåœ¨ç°ä»£ Transformer å®ç°ä¸­ï¼š

```python
# ç”Ÿæˆè¿‡ç¨‹çš„çœŸå®æƒ…å†µ
def generate_next_token(past_tokens):
    # 1. å‡†å¤‡è¾“å…¥ï¼ˆåŒ…æ‹¬æ‰€æœ‰å†å²tokenï¼‰
    inputs = prepare_inputs(past_tokens)  # ["ä»Šå¤©", "å¤©æ°”"]
    
    # 2. é€šè¿‡æ‰€æœ‰ Transformer å±‚
    hidden_states = inputs
    for layer in transformer_layers:
        hidden_states = layer(hidden_states)  # æ¯å±‚éƒ½æ›´æ–°æ‰€æœ‰ä½ç½®çš„çŠ¶æ€
    
    # 3. åªå–æœ€åä¸€ä¸ªä½ç½®çš„çŠ¶æ€ç”¨äºç”Ÿæˆ
    last_position_state = hidden_states[-1]  # ä½ç½®1çš„æœ€ç»ˆçŠ¶æ€
    
    # 4. è®¡ç®—ä¸‹ä¸€ä¸ªtokençš„æ¦‚ç‡
    logits = output_projection(last_position_state)
    next_token = sample(logits)
    
    return next_token
```

#### Decoder State çš„æœ¬è´¨

```mermaid
graph LR
    subgraph "ä½ç½® 0 çš„æ¼”åŒ–"
        A0[ä»Šå¤© + pos0] --> B0[Layer1å¤„ç†] --> C0[Layer2å¤„ç†] --> D0[æœ€ç»ˆçŠ¶æ€0]
    end
    
    subgraph "ä½ç½® 1 çš„æ¼”åŒ–"  
        A1[å¤©æ°” + pos1] --> B1[Layer1å¤„ç†] --> C1[Layer2å¤„ç†] --> D1[æœ€ç»ˆçŠ¶æ€1]
    end
    
    subgraph "äº¤äº’å½±å“"
        B0 -.-> B1
        C0 -.-> C1
    end
    
    D1 --> E[ç”Ÿæˆä½ç½®2çš„æŸ¥è¯¢Q]
```

**Decoder State åŒ…å«äº†**ï¼š
1. **å½“å‰ token çš„è¯­ä¹‰ä¿¡æ¯**
2. **ä½ç½®ä¿¡æ¯**  
3. **ä¸æ‰€æœ‰å†å² token çš„äº¤äº’ç»“æœ**
4. **ç»è¿‡å¤šå±‚æŠ½è±¡åçš„é«˜çº§è¡¨ç¤º**

#### ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ

```python
# Decoder State çš„ä½œç”¨
decoder_state_at_position_1 åŒ…å«äº†ï¼š
- "å¤©æ°”" è¿™ä¸ªè¯çš„è¯­ä¹‰
- å®ƒåœ¨ä½ç½®1çš„ä½ç½®ä¿¡æ¯  
- å®ƒä¸"ä»Šå¤©"çš„å…³ç³»ï¼ˆé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å­¦åˆ°ï¼‰
- ç»è¿‡å¤šå±‚å¤„ç†åçš„æŠ½è±¡è¡¨ç¤º

# ç”¨è¿™ä¸ªçŠ¶æ€è®¡ç®— Qï¼Œå°±æ˜¯åœ¨é—®ï¼š
Q = "åŸºäºæˆ‘ç°åœ¨æŒæ¡çš„æ‰€æœ‰ä¿¡æ¯ï¼ˆä»Šå¤©+å¤©æ°”çš„ç»„åˆè¯­ä¹‰ï¼‰ï¼Œ
     æˆ‘åœ¨ä¸‹ä¸€ä¸ªä½ç½®æƒ³è¦è¡¨è¾¾ä»€ä¹ˆï¼Ÿ"
```

#### å†…å­˜ä½¿ç”¨å¯¹æ¯”

```python
# ä¸ä½¿ç”¨KV Cacheï¼ˆæ¯æ¬¡é‡æ–°è®¡ç®—ï¼‰
ç¬¬1æ­¥: è®¡ç®— Qâ‚, Kâ‚€, Vâ‚€                    # 1ä¸ªtokençš„è®¡ç®—
ç¬¬2æ­¥: è®¡ç®— Qâ‚‚, Kâ‚€, Kâ‚, Vâ‚€, Vâ‚            # 2ä¸ªtokençš„è®¡ç®—ï¼ˆé‡å¤äº†Kâ‚€,Vâ‚€ï¼‰
ç¬¬3æ­¥: è®¡ç®— Qâ‚ƒ, Kâ‚€, Kâ‚, Kâ‚‚, Vâ‚€, Vâ‚, Vâ‚‚    # 3ä¸ªtokençš„è®¡ç®—ï¼ˆé‡å¤äº†å‰é¢æ‰€æœ‰K,Vï¼‰

# ä½¿ç”¨KV Cache
ç¬¬1æ­¥: è®¡ç®— Qâ‚, Kâ‚€, Vâ‚€; ç¼“å­˜ Kâ‚€, Vâ‚€
ç¬¬2æ­¥: è®¡ç®— Qâ‚‚, Kâ‚, Vâ‚; ä»ç¼“å­˜è¯»å– Kâ‚€, Vâ‚€; ç¼“å­˜ Kâ‚, Vâ‚  
ç¬¬3æ­¥: è®¡ç®— Qâ‚ƒ, Kâ‚‚, Vâ‚‚; ä»ç¼“å­˜è¯»å–æ‰€æœ‰å†å² K, V; ç¼“å­˜ Kâ‚‚, Vâ‚‚
```

æ‰€ä»¥ `decoder_state_at_position` å®é™…ä¸Šæ˜¯**ç»è¿‡å®Œæ•´ Transformer å¤„ç†çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥è¡¨ç¤º**ï¼Œå®ƒèåˆäº†å½“å‰ä½ç½®çš„ä¿¡æ¯å’Œæ‰€æœ‰å†å²ä¿¡æ¯çš„äº¤äº’ç»“æœã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆï¼š
- **K, V ç¼“å­˜**ï¼šé¿å…é‡å¤è®¡ç®—å†å² token çš„é”®å€¼ä¿¡æ¯
- **Q ä¸ç¼“å­˜**ï¼šå› ä¸ºæ¯ä¸ªæ–° token çš„æŸ¥è¯¢éœ€æ±‚éƒ½æ˜¯å…¨æ–°çš„ï¼Œå¿…é¡»é‡æ–°è®¡ç®—

## æ€§èƒ½ç‰¹ç‚¹

### ğŸš€ é«˜é€Ÿæ€§èƒ½

#### PagedAttention - è§£å†³ KV Cache å†…å­˜é—®é¢˜

**PagedAttention** æ˜¯ vLLM çš„æ ¸å¿ƒåˆ›æ–°ï¼Œå®ƒåƒæ“ä½œç³»ç»Ÿç®¡ç†å†…å­˜ä¸€æ ·ç®¡ç† KV Cacheï¼š

```mermaid
graph TB
    subgraph "PagedAttention å†…å­˜ç®¡ç†"
        A[ç‰©ç†å†…å­˜æ± ] --> B[é¡µé¢1: 16 tokens]
        A --> C[é¡µé¢2: 16 tokens]
        A --> D[é¡µé¢3: 16 tokens]
        A --> E[é¡µé¢4: 16 tokens]
        
        subgraph "è¯·æ±‚1 (100 tokens)"
            F[è™šæ‹Ÿåœ°å€] --> G[é¡µé¢1,2,3,4,5,6,7]
        end
        
        subgraph "è¯·æ±‚2 (50 tokens)"  
            H[è™šæ‹Ÿåœ°å€] --> I[é¡µé¢8,9,10]
        end
        
        subgraph "è¯·æ±‚3 (200 tokens)"
            J[è™šæ‹Ÿåœ°å€] --> K[é¡µé¢11,12,13,...]
        end
    end
    
    G --> B
    G --> C
    I --> D
    K --> E
```

**PagedAttention çš„ä¼˜åŠ¿**ï¼š

| ç‰¹æ€§ | ä¼ ç»Ÿæ–¹æ³• | PagedAttention |
|:-----|:---------|:---------------|
| **å†…å­˜åˆ†é…** | é¢„åˆ†é…å›ºå®šå¤§å° | æŒ‰éœ€åˆ†é…é¡µé¢ |
| **å†…å­˜åˆ©ç”¨ç‡** | 30-40% | 90%+ |
| **å†…å­˜ç¢ç‰‡** | ä¸¥é‡ | å‡ ä¹æ— ç¢ç‰‡ |
| **æ”¯æŒåºåˆ—é•¿åº¦** | å—é¢„åˆ†é…é™åˆ¶ | åŠ¨æ€æ‰©å±• |
| **å¹¶å‘è¯·æ±‚æ•°** | å—å†…å­˜æµªè´¹é™åˆ¶ | æ˜¾è‘—æå‡ |

#### è¿ç»­æ‰¹å¤„ç†ï¼ˆContinuous Batchingï¼‰

**ä¼ ç»Ÿæ‰¹å¤„ç† vs è¿ç»­æ‰¹å¤„ç†**ï¼š

```mermaid
gantt
    title æ‰¹å¤„ç†å¯¹æ¯” (æ—¶é—´è½´)
    dateFormat X
    axisFormat %s
    
    section ä¼ ç»Ÿæ‰¹å¤„ç†
    è¯·æ±‚A(é•¿) :done, trad_a, 0, 10
    è¯·æ±‚B(çŸ­) :done, trad_b, 0, 3  
    è¯·æ±‚C(çŸ­) :done, trad_c, 0, 2
    GPUç©ºé—² :crit, trad_idle, 2, 10
    ç­‰å¾…æ–°æ‰¹æ¬¡ :crit, trad_wait, 10, 12
    æ–°æ‰¹æ¬¡å¼€å§‹ :trad_new, 12, 15
    
    section è¿ç»­æ‰¹å¤„ç†  
    è¯·æ±‚A(é•¿) :done, cont_a, 0, 10
    è¯·æ±‚B(çŸ­) :done, cont_b, 0, 3
    è¯·æ±‚C(çŸ­) :done, cont_c, 0, 2  
    è¯·æ±‚D :done, cont_d, 2, 7
    è¯·æ±‚E :done, cont_e, 3, 8
    è¯·æ±‚F :done, cont_f, 7, 12
```

**è¿ç»­æ‰¹å¤„ç†çš„å·¥ä½œæœºåˆ¶**ï¼š

1. **åŠ¨æ€è°ƒåº¦**ï¼šä¸€æ—¦æœ‰è¯·æ±‚å®Œæˆï¼Œç«‹å³ç”¨æ–°è¯·æ±‚å¡«è¡¥
2. **å†…å­˜å¤ç”¨**ï¼šå®Œæˆçš„è¯·æ±‚é‡Šæ”¾çš„ KV Cache ç«‹å³å¯ç”¨
3. **è´Ÿè½½å‡è¡¡**ï¼šé•¿çŸ­è¯·æ±‚æ··åˆï¼Œä¿æŒ GPU åˆ©ç”¨ç‡

#### å…¶ä»–æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯

**æ¨æµ‹è§£ç ï¼ˆSpeculative Decodingï¼‰**ï¼š
```mermaid
sequenceDiagram
    participant D as å°æ¨¡å‹(è‰ç¨¿)
    participant T as å¤§æ¨¡å‹(ç›®æ ‡)  
    participant V as éªŒè¯å™¨
    participant O as è¾“å‡º
    
    Note over D,O: æ¨æµ‹é˜¶æ®µ
    D->>D: å¿«é€Ÿç”Ÿæˆå€™é€‰: [token1, token2, token3]
    
    Note over D,O: éªŒè¯é˜¶æ®µ  
    D->>T: å‘é€å€™é€‰åºåˆ—
    T->>V: å¹¶è¡ŒéªŒè¯æ‰€æœ‰å€™é€‰
    V->>V: token1 âœ“, token2 âœ“, token3 âœ—
    
    Note over D,O: è¾“å‡ºé˜¶æ®µ
    V->>O: æ¥å— [token1, token2]
    V->>D: ä» token2 ä½ç½®ç»§ç»­ç”Ÿæˆ
```

**é‡åŒ–æŠ€æœ¯å¯¹æ¯”**ï¼š

| é‡åŒ–æ–¹æ³• | æ¨¡å‹å¤§å° | æ¨ç†é€Ÿåº¦ | ç²¾åº¦æŸå¤± | é€‚ç”¨åœºæ™¯ |
|:---------|:---------|:---------|:---------|:---------|
| **FP16** | åŸºå‡† | åŸºå‡† | æ—  | é«˜ç²¾åº¦è¦æ±‚ |
| **INT8** | 50% | 1.5-2x | å¾ˆå° | å¹³è¡¡æ€§èƒ½ç²¾åº¦ |  
| **INT4** | 25% | 2-3x | å° | èµ„æºå—é™ |
| **GPTQ** | 25% | 2-4x | å¾ˆå° | ç”Ÿäº§ç¯å¢ƒ |
| **AWQ** | 25% | 2-4x | æå° | é«˜è´¨é‡è¦æ±‚ |

### ğŸ”§ çµæ´»æ˜“ç”¨

#### åˆ†å¸ƒå¼æ¨ç†ç­–ç•¥

```mermaid
graph TB
    subgraph "å¼ é‡å¹¶è¡Œ (Tensor Parallelism)"
        A[æ¨¡å‹å±‚] --> B[æƒé‡çŸ©é˜µåˆ†å‰²]
        B --> C[GPU 1: æƒé‡å—1]  
        B --> D[GPU 2: æƒé‡å—2]
        B --> E[GPU 3: æƒé‡å—3]
        C --> F[ç»“æœèšåˆ]
        D --> F
        E --> F
    end
    
    subgraph "æµæ°´çº¿å¹¶è¡Œ (Pipeline Parallelism)"  
        G[è¾“å…¥] --> H[GPU 1: å±‚1-4]
        H --> I[GPU 2: å±‚5-8] 
        I --> J[GPU 3: å±‚9-12]
        J --> K[è¾“å‡º]
    end
```

## æ ¸å¿ƒæ¶æ„

```mermaid
graph TB
    A[å®¢æˆ·ç«¯è¯·æ±‚] --> B[vLLM API æœåŠ¡å™¨]
    B --> C[è¯·æ±‚è°ƒåº¦å™¨]
    C --> D[è¿ç»­æ‰¹å¤„ç†å¼•æ“]
    D --> E[PagedAttention å†…å­˜ç®¡ç†]
    E --> F[æ¨¡å‹æ‰§è¡Œå¼•æ“]
    
    subgraph "KV Cache ç®¡ç†"
        E --> E1[å†…å­˜é¡µé¢æ± ]
        E1 --> E2[åŠ¨æ€åˆ†é…]
        E2 --> E3[ç¢ç‰‡æ•´ç†]
    end
    
    subgraph "æ‰§è¡Œä¼˜åŒ–"
        F --> G[CUDA/HIP å›¾]
        G --> H[é‡åŒ–å¤„ç†]
        H --> I[ä¼˜åŒ–å†…æ ¸]
        I --> J[æ¨æµ‹è§£ç ]
    end
    
    J --> K[æ¨ç†ç»“æœ]
    K --> L[æµå¼è¾“å‡º]
    L --> M[å®¢æˆ·ç«¯å“åº”]
```

## å‚è€ƒèµ„æ–™

- [vLLM å‘å¸ƒåšå®¢](https://blog.vllm.ai/2023/06/20/vllm.html) - PagedAttention ä»‹ç»
- [vLLM è®ºæ–‡](https://arxiv.org/abs/2309.06180) (SOSP 2023)
- [è¿ç»­æ‰¹å¤„ç†å¦‚ä½•åœ¨ LLM æ¨ç†ä¸­å®ç° 23 å€ååé‡æå‡](https://www.anyscale.com/blog/continuous-batching-llm-inference) - Cade Daniel ç­‰äºº
- vLLM Meetups


