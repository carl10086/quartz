
## ç¯å¢ƒé…ç½®

### CUDA ç¯å¢ƒ
```mermaid
graph TD
    A[CUDA ç¯å¢ƒ] --> B[CUDA 12.4]
    A --> C[PyTorch 2.5.1+cu124]
    A --> D[onnxruntime-gpu 1.20.0]
    A --> E[cuDNN 9.x]
```

### ä¾èµ–ç‰ˆæœ¬
```python
# æ ¸å¿ƒä¾èµ–
torch>=2.0.0,<2.6.0
torchvision>=0.15.0,<0.21.0
torchaudio>=2.0.0,<2.6.0
onnxruntime-gpu==1.20.0
optimum[onnxruntime-gpu]>=1.12.0,<1.20.0
```

## æ¨¡å‹è½¬æ¢æµç¨‹

### 1. è½¬æ¢è„šæœ¬
```bash
#!/bin/bash
# official_convert.sh

# é…ç½®è·¯å¾„
ORIGINAL_MODEL="/users/yusizhen/models/vad_turn"
OUTPUT_BASE="./converted_models"
ONNX_OUTPUT="$OUTPUT_BASE/onnx"

# è½¬æ¢ä¸ºONNX
optimum-cli export onnx \
    --model "$ORIGINAL_MODEL" \
    --task sequence-classification \
    --framework pt \
    --opset 14 \
    --atol 1e-3 \
    "$ONNX_OUTPUT"
```

### 2. æ€§èƒ½ä¼˜åŒ–
```mermaid
graph LR
    A[æ¨¡å‹è½¬æ¢] --> B[ONNX Runtime]
    B --> C[CUDA æ‰§è¡Œ]
    B --> D[CPU æ‰§è¡Œ]
    C --> E[æ€§èƒ½ä¼˜åŒ–]
    E --> F[å†…å­˜ä¼˜åŒ–]
    E --> G[è®¡ç®—ä¼˜åŒ–]
```

## æ¨ç†æ€§èƒ½

### æµ‹è¯•ç»“æœ
- PyTorch æ¨ç†æ—¶é—´: 20.32ms
- ONNX æ¨ç†æ—¶é—´: 9.15ms
- åŠ é€Ÿæ¯”: 2.22x

### ç²¾åº¦éªŒè¯
- é¢„æµ‹ç»“æœå®Œå…¨ä¸€è‡´
- Logits æœ€å¤§å·®å¼‚: 0.003769
- åˆ†ç±»å‡†ç¡®ç‡: 100%

## ä»£ç å®ç°

### 1. æ¨¡å‹åŠ è½½
```python
# åŠ è½½ ONNX æ¨¡å‹
onnx_model = ORTModelForSequenceClassification.from_pretrained(
    onnx_path,
    provider="CUDAExecutionProvider",
    local_files_only=True
)
```

### 2. æ¨ç†å®ç°
```python
# å‡†å¤‡è¾“å…¥
inputs = tokenizer(test_text, return_tensors="pt", truncation=True, padding=True)
inputs = {k: v.cuda() for k, v in inputs.items()}

# ONNX æ¨ç†
onnx_output = onnx_model(**inputs)
onnx_logits = onnx_output.logits
onnx_pred = torch.argmax(onnx_logits, dim=-1).cpu().numpy()[0]
```

## REFER

### ä¾èµ–

```python
# =============================================================================
# æ·±åº¦å­¦ä¹ æ ¸å¿ƒæ¡†æ¶
# =============================================================================
--extra-index-url https://download.pytorch.org/whl/cu124
torch>=2.0.0,<2.6.0
torchvision>=0.15.0,<0.21.0
torchaudio>=2.0.0,<2.6.0

# =============================================================================
# Transformersç”Ÿæ€ç³»ç»Ÿ
# =============================================================================
transformers>=4.30.0,<4.50.0
accelerate>=0.20.0,<0.30.0
tokenizers>=0.13.0,<0.20.0
safetensors>=0.3.0,<0.5.0
huggingface-hub>=0.16.0,<0.25.0

# =============================================================================
# æ•°æ®å¤„ç†å’Œå·¥å…·
# =============================================================================
datasets>=2.10.0,<2.20.0
sentencepiece>=0.1.99,<0.2.0
protobuf>=3.20.0,<4.0.0
numpy>=1.21.0,<2.0.0
tiktoken>=0.4.0,<0.8.0

# =============================================================================
# æ¨ç†ä¼˜åŒ–å·¥å…·
# ============================================================================
onnxruntime-gpu>=1.18.1
optimum[onnxruntime-gpu]>=1.12.0,<1.20.0

# =============================================================================
# WebæœåŠ¡æ¡†æ¶
# =============================================================================
fastapi>=0.95.0,<0.110.0
uvicorn>=0.20.0,<0.30.0
pydantic>=2.0.0,<3.0.0

# =============================================================================
# éŸ³é¢‘å¤„ç†
# =============================================================================
librosa>=0.10.0,<0.11.0

# =============================================================================
# è¯­éŸ³è¯†åˆ«ï¼ˆå¯é€‰ï¼Œå¦‚æœéœ€è¦çš„è¯ï¼‰
# =============================================================================
# funasr>=1.0.0

# =============================================================================
# æ³¨æ„ï¼šTensorRTå»ºè®®å•ç‹¬å®‰è£…
tensorrt  # è¯·åœ¨ç¯å¢ƒè®¾ç½®å®Œæˆåå•ç‹¬å®‰è£…
# =============================================================================
```


### ä»£ç 

```python
# correct_onnx_test.py
import time
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from optimum.onnxruntime import ORTModelForSequenceClassification
import onnxruntime as ort

# è·¯å¾„é…ç½®
original_path = "/users/yusizhen/models/vad_turn"
onnx_path = "/group-shared/models/trained_models/turn-detection-1.5B-250605-onnx"

print("ğŸ§ª ONNX GPU æ€§èƒ½æµ‹è¯•...")

# æ£€æŸ¥ CUDA å¯ç”¨æ€§
print("\nğŸ” æ£€æŸ¥ CUDA å¯ç”¨æ€§...")
cuda_available = torch.cuda.is_available()
print(f"CUDA æ˜¯å¦å¯ç”¨: {'âœ…' if cuda_available else 'âŒ'}")

# æ£€æŸ¥ ONNX Runtime æ‰§è¡Œæä¾›ç¨‹åº
print("\nğŸ” æ£€æŸ¥ ONNX Runtime æ‰§è¡Œæä¾›ç¨‹åº...")
available_providers = ort.get_available_providers()
print(f"å¯ç”¨çš„æ‰§è¡Œæä¾›ç¨‹åº: {available_providers}")

# 1. Tokenizer åŠ è½½
print("\nğŸ“¦ åŠ è½½ tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(
    original_path,
    local_files_only=True,
    trust_remote_code=True,
    use_fast=False  # ä½¿ç”¨æ…¢é€Ÿ tokenizer
)
print("âœ… Tokenizer åŠ è½½æˆåŠŸ")

# 2. PyTorch æ¨¡å‹åŠ è½½
print("\nğŸ“¦ åŠ è½½ PyTorch æ¨¡å‹...")
pytorch_model = AutoModelForSequenceClassification.from_pretrained(
    original_path,
    local_files_only=True,
    trust_remote_code=True
).cuda()
pytorch_model.eval()
print("âœ… PyTorch æ¨¡å‹åŠ è½½æˆåŠŸ")

# 3. ONNX æ¨¡å‹åŠ è½½
print("\nğŸ“¦ åŠ è½½ ONNX æ¨¡å‹...")
onnx_model = ORTModelForSequenceClassification.from_pretrained(
    onnx_path,
    provider="CUDAExecutionProvider",
    local_files_only=True
)
print("âœ… ONNX æ¨¡å‹åŠ è½½æˆåŠŸ")
print(f"ONNX æ¨¡å‹ä½¿ç”¨çš„æ‰§è¡Œæä¾›ç¨‹åº: {onnx_model.model.get_providers()}")

# 4. æµ‹è¯•æ¨ç†
test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºéªŒè¯æ¨¡å‹è½¬æ¢æ•ˆæœ"
print(f"\nğŸ§ª æµ‹è¯•æ–‡æœ¬: {test_text}")

# å‡†å¤‡è¾“å…¥
inputs = tokenizer(test_text, return_tensors="pt", truncation=True, padding=True)
inputs = {k: v.cuda() for k, v in inputs.items()}

# æµ‹è¯• PyTorch
print("\nğŸŒ æµ‹è¯• PyTorch æ¨ç†...")
with torch.no_grad():
    pytorch_output = pytorch_model(**inputs)
pytorch_logits = pytorch_output.logits
pytorch_pred = torch.argmax(pytorch_logits, dim=-1).cpu().numpy()[0]

# æµ‹è¯• ONNX
print("ğŸš€ æµ‹è¯• ONNX æ¨ç†...")
onnx_output = onnx_model(**inputs)
onnx_logits = onnx_output.logits
onnx_pred = torch.argmax(onnx_logits, dim=-1).cpu().numpy()[0]

# éªŒè¯ç²¾åº¦
print("\nğŸ” ç²¾åº¦éªŒè¯:")
print(f"PyTorch é¢„æµ‹: {pytorch_pred}")
print(f"ONNX é¢„æµ‹: {onnx_pred}")
print(f"é¢„æµ‹ä¸€è‡´: {'âœ…' if pytorch_pred == onnx_pred else 'âŒ'}")

# è®¡ç®— logits å·®å¼‚
pytorch_logits = pytorch_logits.cpu()  # å°† PyTorch logits ç§»åˆ° CPU
onnx_logits = onnx_logits.cpu()  # ç¡®ä¿ ONNX logits ä¹Ÿåœ¨ CPU ä¸Š
max_diff = torch.max(torch.abs(pytorch_logits - onnx_logits)).item()
print(f"Logits æœ€å¤§å·®å¼‚: {max_diff:.6f}")

# æ€§èƒ½æµ‹è¯•
print("\nâš¡ æ€§èƒ½æµ‹è¯• (10æ¬¡æ¨ç†å¹³å‡):")
# PyTorch
start = time.time()
for _ in range(10):
    with torch.no_grad():
        _ = pytorch_model(**inputs)
pytorch_time = (time.time() - start) / 10 * 1000

# ONNX
start = time.time()
for _ in range(10):
    _ = onnx_model(**inputs)
onnx_time = (time.time() - start) / 10 * 1000

print(f"PyTorch: {pytorch_time:.2f}ms")
print(f"ONNX:    {onnx_time:.2f}ms")
print(f"åŠ é€Ÿæ¯”:   {pytorch_time/onnx_time:.2f}x")

print(f"\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
```

