

# ä»ç¼“æ…¢çš„AIåº”ç”¨åˆ°é—ªç”µèˆ¬çš„å·¥ä½œæµç¨‹ï¼šLangGraphå¼‚æ­¥ç¼–ç¨‹å®Œå…¨æŒ‡å—

ä»¥ä¸‹å†…å®¹ æ€»ç»“è‡ª: https://nishant-mishra.medium.com/why-i-switched-to-async-langchain-and-langgraph-and-you-should-too-c30635c9cf19

## å¼•è¨€ï¼šæ€§èƒ½é©å‘½

ä¸Šå‘¨ï¼Œå½“æˆ‘åœ¨è°ƒè¯•åº”ç”¨ä¸­åˆä¸€ä¸ªè¶…æ—¶é”™è¯¯æ—¶ï¼Œæˆ‘çªç„¶æ„è¯†åˆ°ï¼šæˆ‘ä¸€ç›´åœ¨ç”¨é”™è¯¯çš„æ–¹å¼åšäº‹ã€‚æˆ‘çš„ç”¨æˆ·éœ€è¦ç­‰å¾…60å¤šç§’æ‰èƒ½å®Œæˆç®€å•çš„æ–‡æœ¬åˆ†æï¼ŒæœåŠ¡å™¨æŒç»­æ»¡è´Ÿè·è¿è¡Œï¼ŒAPIç§¯åˆ†æ¶ˆè€—å¾—åƒæ²¡æœ‰æ˜å¤©ä¸€æ ·ã€‚

ç„¶åæˆ‘å‘ç°äº†å¼‚æ­¥LangChainå’ŒLangGraphã€‚ç»“æœå¦‚ä½•ï¼Ÿ

### æ€§èƒ½å¯¹æ¯”ä¸€è§ˆ

| æŒ‡æ ‡ | åŒæ­¥æ–¹å¼ | å¼‚æ­¥æ–¹å¼ | æ”¹è¿›å¹…åº¦ |
|:---|:---:|:---:|:---:|
| å•æ¬¡å“åº”æ—¶é—´ | 30ç§’ | 3ç§’ | **90%å‡å°‘** |
| å¹¶å‘å¤„ç†èƒ½åŠ› | 1ä¸ªç”¨æˆ· | 50ä¸ªç”¨æˆ· | **5000%æå‡** |
| æœåŠ¡å™¨èµ„æºåˆ©ç”¨ç‡ | 20% | 85% | **325%æå‡** |
| APIæˆæœ¬æ•ˆç‡ | åŸºå‡† | èŠ‚çœ60% | **æ˜¾è‘—é™ä½** |

## åŸºç¡€æ¦‚å¿µè§£æ

### LangChainï¼šAIå·¥ä½œæµçš„ç®¡å®¶

å½“æˆ‘ç¬¬ä¸€æ¬¡å¬åˆ°"LangChain"æ—¶ï¼Œæˆ‘ä»¥ä¸ºè¿™æ˜¯æŸç§åŒºå—é“¾æŠ€æœ¯ã€‚äº‹å®ä¸Šï¼Œå®ƒæ¯”é‚£é…·å¤šäº†ã€‚

```mermaid
graph TD
    A[ç”¨æˆ·è¯·æ±‚] --> B[LangChainå¤„ç†å±‚]
    B --> C[APIè°ƒç”¨ç®¡ç†]
    B --> D[å¯¹è¯è®°å¿†ç®¡ç†]
    B --> E[æç¤ºæ¨¡æ¿å¤„ç†]
    B --> F[è¾“å‡ºè§£æ]
    C --> G[AIæ¨¡å‹å“åº”]
    D --> G
    E --> G
    F --> H[ç»“æ„åŒ–è¾“å‡º]
    G --> H
    H --> I[è¿”å›ç”¨æˆ·]
```

**LangChainæ ¸å¿ƒä¼˜åŠ¿ï¼š**
- ğŸ”— ç»Ÿä¸€çš„APIæ¥å£ç®¡ç†
- ğŸ’¾ æ™ºèƒ½çš„å¯¹è¯ä¸Šä¸‹æ–‡ä¿æŒ
- ğŸ“ çµæ´»çš„æç¤ºæ¨¡æ¿ç³»ç»Ÿ
- ğŸ› ï¸ å¤§å¹…å‡å°‘æ ·æ¿ä»£ç 
- ğŸ”„ æ”¯æŒå¤šç§AIæ¨¡å‹åˆ‡æ¢

### LangGraphï¼šå¯è§†åŒ–AIå·¥ä½œæµç¼–æ’

LangGraphè®©ä½ åƒç”»æµç¨‹å›¾ä¸€æ ·æ„å»ºAIåº”ç”¨ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œä½ åœ¨ç™½æ¿ä¸Šç”»å‡ºçš„é‚£äº›æ–¹æ¡†å’Œç®­å¤´ï¼Œç°åœ¨å¯ä»¥ç›´æ¥å˜æˆå¯æ‰§è¡Œçš„ä»£ç ã€‚

```mermaid
graph LR
    A[å®¢æˆ·åé¦ˆè¾“å…¥] --> B{æ™ºèƒ½åˆ†ç±»}
    B -->|æŠ•è¯‰| C[æƒ…æ„Ÿåˆ†æ]
    B -->|è¡¨æ‰¬| D[å…³é”®è¯æå–]
    B -->|è¯¢é—®| E[æ„å›¾è¯†åˆ«]
    C --> F[ç”Ÿæˆå›å¤ç­–ç•¥]
    D --> F
    E --> F
    F --> G[è‡ªåŠ¨å›å¤]
    G --> H[äººå·¥å®¡æ ¸é˜Ÿåˆ—]
```

## å¼‚æ­¥ç¼–ç¨‹çš„å¯ç¤º

### åŒæ­¥ vs å¼‚æ­¥ï¼šå’–å•¡åº—çš„æ¯”å–»

æƒ³è±¡ä¸¤ç§å’–å•¡åº—çš„æœåŠ¡æ¨¡å¼ï¼š

**åŒæ­¥å’–å•¡åº—ï¼ˆä¼ ç»Ÿæ–¹å¼ï¼‰ï¼š**
```python
def synchronous_coffee_shop():
    for customer in customers:
        take_order(customer)      # æ¥å•ï¼š30ç§’
        make_coffee(customer)     # åˆ¶ä½œï¼š3åˆ†é’Ÿ  
        serve_coffee(customer)    # ä¸Šå’–å•¡ï¼š10ç§’
        # ä¸‹ä¸€ä¸ªå®¢æˆ·å¿…é¡»ç­‰å¾…ä¸Šä¸€ä¸ªå®Œå…¨ç»“æŸ
```

**å¼‚æ­¥å’–å•¡åº—ï¼ˆç°ä»£æ–¹å¼ï¼‰ï¼š**
```python
async def asynchronous_coffee_shop():
    tasks = []
    for customer in customers:
        # åŒæ—¶å¤„ç†å¤šä¸ªè®¢å•
        task = asyncio.create_task(process_customer(customer))
        tasks.append(task)
    
    # æ‰€æœ‰è®¢å•å¹¶è¡Œå¤„ç†
    results = await asyncio.gather(*tasks)
```

### æ€§èƒ½å·®å¼‚å¯è§†åŒ–

```mermaid
gantt
    title å¤„ç†5ä¸ªè¯·æ±‚çš„æ—¶é—´å¯¹æ¯”
    dateFormat X
    axisFormat %s
    
    section åŒæ­¥å¤„ç†
    è¯·æ±‚1    :0, 3
    è¯·æ±‚2    :3, 6  
    è¯·æ±‚3    :6, 9
    è¯·æ±‚4    :9, 12
    è¯·æ±‚5    :12, 15
    
    section å¼‚æ­¥å¤„ç†
    è¯·æ±‚1    :0, 3
    è¯·æ±‚2    :0, 3
    è¯·æ±‚3    :0, 3  
    è¯·æ±‚4    :0, 3
    è¯·æ±‚5    :0, 3
```

## æ··åˆèŠ‚ç‚¹æœºåˆ¶è¯¦è§£

### LangGraphçš„æ™ºèƒ½èŠ‚ç‚¹å¤„ç†

è¿™æ˜¯LangGraphæœ€å¼ºå¤§çš„ç‰¹æ€§ä¹‹ä¸€ï¼š**ä½ å¯ä»¥åœ¨åŒä¸€ä¸ªå·¥ä½œæµä¸­æ··åˆä½¿ç”¨åŒæ­¥å’Œå¼‚æ­¥èŠ‚ç‚¹**ã€‚

```python
from langgraph.graph import StateGraph, END
from typing import TypedDict
import asyncio

class WorkflowState(TypedDict):
    input_text: str
    validation_result: bool
    ai_analysis: str
    processed_data: dict
    final_result: str

# 1. åŒæ­¥èŠ‚ç‚¹ï¼šæ•°æ®éªŒè¯ï¼ˆå¿«é€Ÿæ“ä½œï¼‰
def validate_input(state: WorkflowState) -> dict:
    """å¿«é€Ÿçš„æ•°æ®éªŒè¯ï¼Œä¸éœ€è¦å¼‚æ­¥"""
    text = state["input_text"]
    is_valid = len(text.strip()) > 0 and len(text) < 10000
    word_count = len(text.split())
    
    return {
        "validation_result": is_valid,
        "word_count": word_count
    }

# 2. å¼‚æ­¥èŠ‚ç‚¹ï¼šAIåˆ†æï¼ˆè€—æ—¶æ“ä½œï¼‰
async def ai_analysis(state: WorkflowState) -> dict:
    """AIè°ƒç”¨å¿…é¡»å¼‚æ­¥"""
    if not state["validation_result"]:
        return {"ai_analysis": "è¾“å…¥æ— æ•ˆï¼Œè·³è¿‡åˆ†æ"}
    
    messages = [
        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬åˆ†æå¸ˆ"),
        HumanMessage(content=f"åˆ†æè¿™æ®µæ–‡æœ¬ï¼š{state['input_text']}")
    ]
    
    response = await llm.ainvoke(messages)
    return {"ai_analysis": response.content}

# 3. åŒæ­¥èŠ‚ç‚¹ï¼šæ•°æ®å¤„ç†ï¼ˆè®¡ç®—å¯†é›†å‹ï¼‰
def process_data(state: WorkflowState) -> dict:
    """çº¯è®¡ç®—æ“ä½œï¼ŒåŒæ­¥å³å¯"""
    analysis = state["ai_analysis"]
    
    # æ¨¡æ‹Ÿä¸€äº›æ•°æ®å¤„ç†
    processed = {
        "sentiment_score": 0.8,
        "key_topics": ["ä¸»é¢˜1", "ä¸»é¢˜2"],
        "confidence": 0.95
    }
    
    return {"processed_data": processed}

# 4. å¼‚æ­¥èŠ‚ç‚¹ï¼šå¤–éƒ¨APIè°ƒç”¨
async def external_api_call(state: WorkflowState) -> dict:
    """è°ƒç”¨å¤–éƒ¨æœåŠ¡"""
    import aiohttp
    
    async with aiohttp.ClientSession() as session:
        async with session.post(
            "https://api.example.com/enrich",
            json=state["processed_data"]
        ) as response:
            enriched_data = await response.json()
    
    return {"final_result": str(enriched_data)}

# æ„å»ºæ··åˆå·¥ä½œæµ
def build_mixed_workflow():
    builder = StateGraph(WorkflowState)
    
    # æ·»åŠ ä¸åŒç±»å‹çš„èŠ‚ç‚¹
    builder.add_node("validate", validate_input)      # åŒæ­¥
    builder.add_node("analyze", ai_analysis)          # å¼‚æ­¥
    builder.add_node("process", process_data)         # åŒæ­¥  
    builder.add_node("enrich", external_api_call)     # å¼‚æ­¥
    
    # å®šä¹‰æ‰§è¡Œæµç¨‹
    builder.set_entry_point("validate")
    builder.add_edge("validate", "analyze")
    builder.add_edge("analyze", "process")
    builder.add_edge("process", "enrich")
    builder.add_edge("enrich", END)
    
    return builder.compile()
```

### LangGraphå†…éƒ¨æ‰§è¡Œæœºåˆ¶

```mermaid
graph TD
    A[ainvokeè°ƒç”¨] --> B{æ£€æŸ¥èŠ‚ç‚¹ç±»å‹}
    B -->|åŒæ­¥å‡½æ•°| C[asyncio.to_threadåŒ…è£…]
    B -->|å¼‚æ­¥å‡½æ•°| D[ç›´æ¥awaitæ‰§è¡Œ]
    C --> E[éé˜»å¡æ‰§è¡Œ]
    D --> F[å¼‚æ­¥æ‰§è¡Œ]
    E --> G[çŠ¶æ€æ›´æ–°]
    F --> G
    G --> H{è¿˜æœ‰ä¸‹ä¸€ä¸ªèŠ‚ç‚¹?}
    H -->|æ˜¯| B
    H -->|å¦| I[è¿”å›æœ€ç»ˆç»“æœ]
    
    style C fill:#e1f5fe
    style D fill:#f3e5f5
    style E fill:#e8f5e8
    style F fill:#fff3e0
```

## å¤„ç†è€—æ—¶åŒæ­¥æ“ä½œ

### é—®é¢˜åœºæ™¯

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œä½ ç»å¸¸ä¼šé‡åˆ°è¿™æ ·çš„æƒ…å†µï¼š

```python
# è¿™äº›æ“ä½œå¾ˆè€—æ—¶ï¼Œä½†ä¸æ”¯æŒå¼‚æ­¥
def heavy_computation(data):
    """CPUå¯†é›†å‹è®¡ç®—ï¼Œéœ€è¦30ç§’"""
    # å¤æ‚çš„æ•°å­¦è¿ç®—ã€å›¾åƒå¤„ç†ã€æ•°æ®æŒ–æ˜ç­‰
    time.sleep(30)  # æ¨¡æ‹Ÿè€—æ—¶æ“ä½œ
    return processed_data

def legacy_database_call(query):
    """è€æ—§çš„åŒæ­¥æ•°æ®åº“è°ƒç”¨ï¼Œéœ€è¦10ç§’"""
    # ä½¿ç”¨ä¸æ”¯æŒå¼‚æ­¥çš„æ•°æ®åº“é©±åŠ¨
    time.sleep(10)
    return query_result

def file_processing(file_path):
    """å¤§æ–‡ä»¶å¤„ç†ï¼Œéœ€è¦60ç§’"""
    # å¤„ç†å¤§å‹æ–‡ä»¶ã€è§†é¢‘è½¬ç ç­‰
    time.sleep(60)
    return processed_file
```

### è§£å†³æ–¹æ¡ˆï¼šçº¿ç¨‹æ± åŒ…è£…

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from functools import partial
import time

class AsyncWrapper:
    """å¼‚æ­¥åŒ…è£…å™¨ï¼Œå¤„ç†è€—æ—¶çš„åŒæ­¥æ“ä½œ"""
    
    def __init__(self, max_workers=4):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
    
    async def run_in_thread(self, func, *args, **kwargs):
        """åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥å‡½æ•°"""
        loop = asyncio.get_event_loop()
        if kwargs:
            func = partial(func, **kwargs)
        return await loop.run_in_executor(self.executor, func, *args)
    
    def __del__(self):
        if hasattr(self, 'executor'):
            self.executor.shutdown(wait=True)

# å…¨å±€å¼‚æ­¥åŒ…è£…å™¨å®ä¾‹
async_wrapper = AsyncWrapper(max_workers=8)

# åŒ…è£…è€—æ—¶çš„åŒæ­¥æ“ä½œ
async def async_heavy_computation(state: dict) -> dict:
    """å°†CPUå¯†é›†å‹è®¡ç®—åŒ…è£…ä¸ºå¼‚æ­¥"""
    data = state["input_data"]
    
    # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œè€—æ—¶æ“ä½œ
    result = await async_wrapper.run_in_thread(heavy_computation, data)
    
    return {"computation_result": result}

async def async_legacy_database(state: dict) -> dict:
    """å°†åŒæ­¥æ•°æ®åº“è°ƒç”¨åŒ…è£…ä¸ºå¼‚æ­¥"""
    query = state["query"]
    
    result = await async_wrapper.run_in_thread(legacy_database_call, query)
    
    return {"db_result": result}

async def async_file_processing(state: dict) -> dict:
    """å°†æ–‡ä»¶å¤„ç†åŒ…è£…ä¸ºå¼‚æ­¥"""
    file_path = state["file_path"]
    
    result = await async_wrapper.run_in_thread(file_processing, file_path)
    
    return {"file_result": result}
```

### é«˜çº§çº¿ç¨‹æ± ç®¡ç†

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Callable, Any
import logging

class AdvancedAsyncWrapper:
    """é«˜çº§å¼‚æ­¥åŒ…è£…å™¨ï¼Œæ”¯æŒæ‰¹é‡å¤„ç†å’Œé”™è¯¯æ¢å¤"""
    
    def __init__(self, max_workers=8, timeout=300):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.timeout = timeout
        self.logger = logging.getLogger(__name__)
    
    async def run_single(self, func: Callable, *args, **kwargs) -> Any:
        """è¿è¡Œå•ä¸ªåŒæ­¥å‡½æ•°"""
        try:
            loop = asyncio.get_event_loop()
            if kwargs:
                func = partial(func, **kwargs)
            
            return await asyncio.wait_for(
                loop.run_in_executor(self.executor, func, *args),
                timeout=self.timeout
            )
        except asyncio.TimeoutError:
            self.logger.error(f"å‡½æ•° {func.__name__} æ‰§è¡Œè¶…æ—¶")
            raise
        except Exception as e:
            self.logger.error(f"å‡½æ•° {func.__name__} æ‰§è¡Œå¤±è´¥: {e}")
            raise
    
    async def run_batch(self, tasks: List[tuple]) -> List[Any]:
        """æ‰¹é‡è¿è¡ŒåŒæ­¥å‡½æ•°"""
        futures = []
        
        for func, args, kwargs in tasks:
            future = self.run_single(func, *args, **kwargs)
            futures.append(future)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*futures, return_exceptions=True)
        
        # å¤„ç†å¼‚å¸¸
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                self.logger.error(f"ä»»åŠ¡ {i} å¤±è´¥: {result}")
        
        return results
    
    async def run_with_progress(self, tasks: List[tuple], 
                               progress_callback=None) -> List[Any]:
        """å¸¦è¿›åº¦å›è°ƒçš„æ‰¹é‡æ‰§è¡Œ"""
        futures = {}
        
        for i, (func, args, kwargs) in enumerate(tasks):
            future = asyncio.create_task(
                self.run_single(func, *args, **kwargs)
            )
            futures[future] = i
        
        results = [None] * len(tasks)
        completed = 0
        
        for future in as_completed(futures.keys()):
            try:
                result = await future
                task_index = futures[future]
                results[task_index] = result
                completed += 1
                
                if progress_callback:
                    progress_callback(completed, len(tasks))
                    
            except Exception as e:
                task_index = futures[future]
                results[task_index] = e
                self.logger.error(f"ä»»åŠ¡ {task_index} å¤±è´¥: {e}")
        
        return results

# ä½¿ç”¨ç¤ºä¾‹
advanced_wrapper = AdvancedAsyncWrapper(max_workers=16, timeout=600)

async def batch_processing_node(state: dict) -> dict:
    """æ‰¹é‡å¤„ç†èŠ‚ç‚¹"""
    files = state["file_list"]
    
    # å‡†å¤‡æ‰¹é‡ä»»åŠ¡
    tasks = [
        (process_single_file, (file_path,), {})
        for file_path in files
    ]
    
    # å¸¦è¿›åº¦çš„æ‰¹é‡æ‰§è¡Œ
    def progress_callback(completed, total):
        print(f"å¤„ç†è¿›åº¦: {completed}/{total} ({completed/total*100:.1f}%)")
    
    results = await advanced_wrapper.run_with_progress(
        tasks, 
        progress_callback=progress_callback
    )
    
    return {"batch_results": results}
```

## å®é™…æ¡ˆä¾‹ï¼šå®Œæ•´çš„æ–‡æœ¬åˆ†æå™¨

### å®Œæ•´çš„ç”Ÿäº§çº§ä»£ç 

```python
import os
import asyncio
import time
import logging
from typing import TypedDict, List, Optional
from concurrent.futures import ThreadPoolExecutor
from functools import partial
import aiohttp
import aiofiles
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage
from langgraph.graph import StateGraph, END

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆå§‹åŒ–LLM
llm = ChatOpenAI(model="gpt-4", temperature=0, max_retries=3)

# çº¿ç¨‹æ± åŒ…è£…å™¨
class AsyncWrapper:
    def __init__(self, max_workers=8):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
    
    async def run_in_thread(self, func, *args, **kwargs):
        loop = asyncio.get_event_loop()
        if kwargs:
            func = partial(func, **kwargs)
        return await loop.run_in_executor(self.executor, func, *args)

async_wrapper = AsyncWrapper()

# çŠ¶æ€å®šä¹‰
class AnalysisState(TypedDict):
    # è¾“å…¥
    text: str
    user_id: str
    request_id: str
    
    # éªŒè¯é˜¶æ®µ
    is_valid: bool
    word_count: int
    language: str
    
    # AIåˆ†æé˜¶æ®µ
    topic_classification: str
    sentiment_analysis: dict
    key_entities: List[str]
    summary: str
    
    # æ•°æ®å¤„ç†é˜¶æ®µ
    processed_metrics: dict
    similarity_scores: List[float]
    
    # å¤–éƒ¨æœåŠ¡é˜¶æ®µ
    enriched_data: dict
    
    # æœ€ç»ˆç»“æœ
    final_analysis: dict
    processing_time: float
    status: str

# 1. åŒæ­¥èŠ‚ç‚¹ï¼šè¾“å…¥éªŒè¯ï¼ˆå¿«é€Ÿæ“ä½œï¼‰
def validate_input(state: AnalysisState) -> dict:
    """å¿«é€ŸéªŒè¯è¾“å…¥æ•°æ®"""
    start_time = time.time()
    
    text = state["text"]
    
    # åŸºæœ¬éªŒè¯
    is_valid = (
        len(text.strip()) > 0 and 
        len(text) < 50000 and
        len(text.split()) >= 3
    )
    
    word_count = len(text.split())
    
    # ç®€å•è¯­è¨€æ£€æµ‹ï¼ˆåŒæ­¥æ“ä½œï¼‰
    language = detect_language_simple(text)
    
    processing_time = time.time() - start_time
    logger.info(f"è¾“å…¥éªŒè¯å®Œæˆï¼Œè€—æ—¶: {processing_time:.3f}ç§’")
    
    return {
        "is_valid": is_valid,
        "word_count": word_count,
        "language": language
    }

def detect_language_simple(text: str) -> str:
    """ç®€å•çš„è¯­è¨€æ£€æµ‹ï¼ˆåŒæ­¥ï¼‰"""
    # ç®€åŒ–çš„è¯­è¨€æ£€æµ‹é€»è¾‘
    chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
    if chinese_chars / len(text) > 0.3:
        return "zh"
    return "en"

# 2. å¼‚æ­¥èŠ‚ç‚¹ï¼šAIåˆ†æï¼ˆè€—æ—¶æ“ä½œï¼‰
async def ai_analysis(state: AnalysisState) -> dict:
    """AIé©±åŠ¨çš„æ–‡æœ¬åˆ†æ"""
    if not state["is_valid"]:
        return {
            "topic_classification": "invalid_input",
            "sentiment_analysis": {"score": 0, "label": "neutral"},
            "key_entities": [],
            "summary": "è¾“å…¥æ— æ•ˆ"
        }
    
    start_time = time.time()
    text = state["text"]
    
    # å¹¶è¡Œæ‰§è¡Œå¤šä¸ªAIä»»åŠ¡
    tasks = [
        classify_topic(text),
        analyze_sentiment(text),
        extract_entities(text),
        generate_summary(text)
    ]
    
    results = await asyncio.gather(*tasks)
    
    processing_time = time.time() - start_time
    logger.info(f"AIåˆ†æå®Œæˆï¼Œè€—æ—¶: {processing_time:.3f}ç§’")
    
    return {
        "topic_classification": results[0],
        "sentiment_analysis": results[1],
        "key_entities": results[2],
        "summary": results[3]
    }

async def classify_topic(text: str) -> str:
    """ä¸»é¢˜åˆ†ç±»"""
    messages = [
        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬åˆ†ç±»å™¨ã€‚è¯·å°†æ–‡æœ¬åˆ†ç±»ä¸ºï¼šæŠ€æœ¯ã€å•†ä¸šã€å¨±ä¹ã€æ•™è‚²ã€å…¶ä»–ä¹‹ä¸€ã€‚åªè¿”å›åˆ†ç±»ç»“æœã€‚"),
        HumanMessage(content=f"åˆ†ç±»è¿™æ®µæ–‡æœ¬ï¼š{text[:1000]}")
    ]
    response = await llm.ainvoke(messages)
    return response.content.strip()

async def analyze_sentiment(text: str) -> dict:
    """æƒ…æ„Ÿåˆ†æ"""
    messages = [
        SystemMessage(content="ä½ æ˜¯æƒ…æ„Ÿåˆ†æä¸“å®¶ã€‚åˆ†ææ–‡æœ¬æƒ…æ„Ÿï¼Œè¿”å›JSONæ ¼å¼ï¼š{\"score\": 0.8, \"label\": \"positive\"}"),
        HumanMessage(content=f"åˆ†ææƒ…æ„Ÿï¼š{text[:1000]}")
    ]
    response = await llm.ainvoke(messages)
    try:
        import json
        return json.loads(response.content)
    except:
        return {"score": 0.0, "label": "neutral"}

async def extract_entities(text: str) -> List[str]:
    """å®ä½“æå–"""
    messages = [
        SystemMessage(content="æå–æ–‡æœ¬ä¸­çš„å…³é”®å®ä½“ï¼ˆäººåã€åœ°åã€æœºæ„åç­‰ï¼‰ï¼Œä»¥JSONæ•°ç»„æ ¼å¼è¿”å›ã€‚"),
        HumanMessage(content=f"æå–å®ä½“ï¼š{text[:1000]}")
    ]
    response = await llm.ainvoke(messages)
    try:
        import json
        return json.loads(response.content)
    except:
        return []

async def generate_summary(text: str) -> str:
    """ç”Ÿæˆæ‘˜è¦"""
    messages = [
        SystemMessage(content="ç”Ÿæˆç®€æ´çš„æ–‡æœ¬æ‘˜è¦ï¼Œä¸è¶…è¿‡100å­—ã€‚"),
        HumanMessage(content=f"æ‘˜è¦ï¼š{text}")
    ]
    response = await llm.ainvoke(messages)
    return response.content.strip()

# 3. åŒæ­¥èŠ‚ç‚¹ï¼šæ•°æ®å¤„ç†ï¼ˆCPUå¯†é›†å‹ï¼‰
async def process_data(state: AnalysisState) -> dict:
    """CPUå¯†é›†å‹æ•°æ®å¤„ç†ï¼ˆä½¿ç”¨çº¿ç¨‹æ± ï¼‰"""
    start_time = time.time()
    
    # å°†CPUå¯†é›†å‹æ“ä½œæ”¾åˆ°çº¿ç¨‹æ± ä¸­
    metrics = await async_wrapper.run_in_thread(
        calculate_text_metrics, 
        state["text"]
    )
    
    similarity_scores = await async_wrapper.run_in_thread(
        calculate_similarity_scores,
        state["text"],
        state["key_entities"]
    )
    
    processing_time = time.time() - start_time
    logger.info(f"æ•°æ®å¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.3f}ç§’")
    
    return {
        "processed_metrics": metrics,
        "similarity_scores": similarity_scores
    }

def calculate_text_metrics(text: str) -> dict:
    """è®¡ç®—æ–‡æœ¬æŒ‡æ ‡ï¼ˆCPUå¯†é›†å‹ï¼‰"""
    import re
    from collections import Counter
    
    # æ¨¡æ‹Ÿå¤æ‚è®¡ç®—
    time.sleep(2)  # æ¨¡æ‹Ÿè€—æ—¶æ“ä½œ
    
    words = re.findall(r'\w+', text.lower())
    word_freq = Counter(words)
    
    return {
        "unique_words": len(word_freq),
        "avg_word_length": sum(len(word) for word in words) / len(words) if words else 0,
        "most_common": word_freq.most_common(10),
        "readability_score": len(words) / len(text.split('.')) if '.' in text else 0
    }

def calculate_similarity_scores(text: str, entities: List[str]) -> List[float]:
    """è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆCPUå¯†é›†å‹ï¼‰"""
    # æ¨¡æ‹Ÿå¤æ‚çš„ç›¸ä¼¼åº¦è®¡ç®—
    time.sleep(1)
    
    import random
    return [random.random() for _ in range(min(len(entities), 10))]

# 4. å¼‚æ­¥èŠ‚ç‚¹ï¼šå¤–éƒ¨APIè°ƒç”¨
async def enrich_with_external_data(state: AnalysisState) -> dict:
    """è°ƒç”¨å¤–éƒ¨æœåŠ¡ä¸°å¯Œæ•°æ®"""
    start_time = time.time()
    
    try:
        # æ¨¡æ‹Ÿè°ƒç”¨å¤–éƒ¨API
        enriched_data = await call_external_api(
            state["topic_classification"],
            state["key_entities"]
        )
    except Exception as e:
        logger.error(f"å¤–éƒ¨APIè°ƒç”¨å¤±è´¥: {e}")
        enriched_data = {"error": str(e)}
    
    processing_time = time.time() - start_time
    logger.info(f"å¤–éƒ¨æ•°æ®ä¸°å¯Œå®Œæˆï¼Œè€—æ—¶: {processing_time:.3f}ç§’")
    
    return {"enriched_data": enriched_data}

async def call_external_api(topic: str, entities: List[str]) -> dict:
    """è°ƒç”¨å¤–éƒ¨API"""
    # æ¨¡æ‹Ÿå¤–éƒ¨APIè°ƒç”¨
    await asyncio.sleep(1)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
    
    return {
        "related_topics": [f"ç›¸å…³ä¸»é¢˜_{i}" for i in range(3)],
        "entity_details": {entity: f"è¯¦æƒ…_{entity}" for entity in entities[:5]},
        "confidence": 0.95
    }

# 5. åŒæ­¥èŠ‚ç‚¹ï¼šç»“æœæ±‡æ€»
def finalize_analysis(state: AnalysisState) -> dict:
    """æ±‡æ€»æœ€ç»ˆåˆ†æç»“æœ"""
    final_analysis = {
        "request_id": state["request_id"],
        "user_id": state["user_id"],
        "input_stats": {
            "word_count": state["word_count"],
            "language": state["language"]
        },
        "analysis_results": {
            "topic": state["topic_classification"],
            "sentiment": state["sentiment_analysis"],
            "entities": state["key_entities"],
            "summary": state["summary"]
        },
        "metrics": state["processed_metrics"],
        "external_data": state["enriched_data"],
        "processing_metadata": {
            "timestamp": time.time(),
            "status": "completed"
        }
    }
    
    return {
        "final_analysis": final_analysis,
        "status": "completed"
    }

# æ„å»ºå®Œæ•´çš„åˆ†æå·¥ä½œæµ
def build_analysis_workflow():
    """æ„å»ºå®Œæ•´çš„æ–‡æœ¬åˆ†æå·¥ä½œæµ"""
    builder = StateGraph(AnalysisState)
    
    # æ·»åŠ æ‰€æœ‰èŠ‚ç‚¹
    builder.add_node("validate", validate_input)           # åŒæ­¥ï¼šå¿«é€ŸéªŒè¯
    builder.add_node("ai_analyze", ai_analysis)            # å¼‚æ­¥ï¼šAIåˆ†æ
    builder.add_node("process", process_data)              # å¼‚æ­¥ï¼šCPUå¯†é›†å‹å¤„ç†
    builder.add_node("enrich", enrich_with_external_data)  # å¼‚æ­¥ï¼šå¤–éƒ¨API
    builder.add_node("finalize", finalize_analysis)        # åŒæ­¥ï¼šç»“æœæ±‡æ€»
    
    # å®šä¹‰æ‰§è¡Œæµç¨‹
    builder.set_entry_point("validate")
    
    # æ·»åŠ æ¡ä»¶è¾¹ï¼šåªæœ‰éªŒè¯é€šè¿‡æ‰ç»§ç»­
    builder.add_conditional_edges(
        "validate",
        lambda state: "ai_analyze" if state["is_valid"] else "finalize",
        {
            "ai_analyze": "ai_analyze",
            "finalize": "finalize"
        }
    )
    
    builder.add_edge("ai_analyze", "process")
    builder.add_edge("process", "enrich")
    builder.add_edge("enrich", "finalize")
    builder.add_edge("finalize", END)
    
    return builder.compile()

# FastAPIåº”ç”¨
app = FastAPI(title="é«˜æ€§èƒ½æ–‡æœ¬åˆ†æAPI", version="2.0.0")

# è¯·æ±‚/å“åº”æ¨¡å‹
class AnalysisRequest(BaseModel):
    text: str = Field(..., min_length=1, max_length=50000)
    user_id: str = Field(..., min_length=1)

class AnalysisResponse(BaseModel):
    request_id: str
    status: str
    final_analysis: Optional[dict] = None
    error: Optional[str] = None
    processing_time: float

# å…¨å±€å·¥ä½œæµå®ä¾‹
analysis_workflow = build_analysis_workflow()

@app.post("/analyze", response_model=AnalysisResponse)
async def analyze_text(request: AnalysisRequest):
    """å¼‚æ­¥æ–‡æœ¬åˆ†æç«¯ç‚¹"""
    import uuid
    
    request_id = str(uuid.uuid4())
    start_time = time.time()
    
    try:
        # å‡†å¤‡åˆå§‹çŠ¶æ€
        initial_state = {
            "text": request.text,
            "user_id": request.user_id,
            "request_id": request_id,
            "processing_time": 0.0,
            "status": "processing"
        }
        
        # æ‰§è¡Œå¼‚æ­¥å·¥ä½œæµ
        result = await analysis_workflow.ainvoke(initial_state)
        
        processing_time = time.time() - start_time
        
        return AnalysisResponse(
            request_id=request_id,
            status=result["status"],
            final_analysis=result["final_analysis"],
            processing_time=processing_time
        )
        
    except Exception as e:
        logger.error(f"åˆ†æå¤±è´¥ {request_id}: {e}")
        processing_time = time.time() - start_time
        
        return AnalysisResponse(
            request_id=request_id,
            status="error",
            error=str(e),
            processing_time=processing_time
        )

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {"status": "healthy", "timestamp": time.time()}

# æ‰¹é‡å¤„ç†ç«¯ç‚¹
@app.post("/analyze/batch")
async def batch_analyze(requests: List[AnalysisRequest]):
    """æ‰¹é‡æ–‡æœ¬åˆ†æ"""
    start_time = time.time()
    
    # åˆ›å»ºå¹¶å‘ä»»åŠ¡
    tasks = []
    for req in requests:
        task = analyze_text(req)
        tasks.append(task)
    
    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰åˆ†æ
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # å¤„ç†å¼‚å¸¸
    processed_results = []
    for result in results:
        if isinstance(result, Exception):
            processed_results.append({
                "status": "error",
                "error": str(result)
            })
        else:
            processed_results.append(result)
    
    total_time = time.time() - start_time
    
    return {
        "batch_id": str(uuid.uuid4()),
        "total_requests": len(requests),
        "results": processed_results,
        "total_processing_time": total_time,
        "average_time_per_request": total_time / len(requests)
    }
```

### å·¥ä½œæµç¨‹å¯è§†åŒ–

```mermaid
graph TD
    A[å¼€å§‹] --> B[validate_input<br/>åŒæ­¥éªŒè¯]
    B --> C{è¾“å…¥æ˜¯å¦æœ‰æ•ˆ?}
    C -->|æ— æ•ˆ| H[finalize_analysis<br/>åŒæ­¥æ±‡æ€»]
    C -->|æœ‰æ•ˆ| D[ai_analysis<br/>å¼‚æ­¥AIåˆ†æ]
    D --> E[process_data<br/>å¼‚æ­¥CPUå¤„ç†]
    E --> F[enrich_with_external_data<br/>å¼‚æ­¥å¤–éƒ¨API]
    F --> G[finalize_analysis<br/>åŒæ­¥æ±‡æ€»]
    G --> H
    H --> I[ç»“æŸ]
    
    style B fill:#e1f5fe
    style D fill:#f3e5f5
    style E fill:#f3e5f5
    style F fill:#f3e5f5
    style G fill:#e1f5fe
    style H fill:#e1f5fe
```

## æœ€ä½³å®è·µä¸æ€§èƒ½ä¼˜åŒ–

### 1. èŠ‚ç‚¹ç±»å‹é€‰æ‹©ç­–ç•¥

```python
class NodeTypeGuide:
    """èŠ‚ç‚¹ç±»å‹é€‰æ‹©æŒ‡å—"""
    
    # âœ… é€‚åˆåŒæ­¥èŠ‚ç‚¹çš„æ“ä½œ
    SYNC_OPERATIONS = [
        "æ•°æ®éªŒè¯å’Œæ¸…æ´—",
        "ç®€å•çš„æ•°å­¦è®¡ç®—",
        "å­—ç¬¦ä¸²å¤„ç†",
        "é…ç½®è¯»å–",
        "æ—¥å¿—è®°å½•",
        "çŠ¶æ€æ£€æŸ¥"
    ]
    
    # âœ… å¿…é¡»å¼‚æ­¥çš„æ“ä½œ
    ASYNC_OPERATIONS = [
        "AIæ¨¡å‹APIè°ƒç”¨",
        "HTTPè¯·æ±‚",
        "æ•°æ®åº“æŸ¥è¯¢ï¼ˆæ”¯æŒå¼‚æ­¥çš„é©±åŠ¨ï¼‰",
        "æ–‡ä»¶I/Oï¼ˆä½¿ç”¨aiofilesï¼‰",
        "æ¶ˆæ¯é˜Ÿåˆ—æ“ä½œ"
    ]
    
    # âš ï¸ éœ€è¦çº¿ç¨‹æ± åŒ…è£…çš„æ“ä½œ
    THREAD_POOL_OPERATIONS = [
        "CPUå¯†é›†å‹è®¡ç®—",
        "å›¾åƒ/è§†é¢‘å¤„ç†",
        "å¤§æ–‡ä»¶è§£æ",
        "åŒæ­¥æ•°æ®åº“æ“ä½œ",
        "ç¬¬ä¸‰æ–¹åŒæ­¥åº“è°ƒç”¨"
    ]
```

### 2. æ€§èƒ½ç›‘æ§ä¸è°ƒä¼˜

```python
import time
import psutil
import asyncio
from functools import wraps
from typing import Dict, List
import logging

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics = {}
        self.logger = logging.getLogger(__name__)
    
    def monitor_node(self, node_name: str):
        """èŠ‚ç‚¹æ€§èƒ½ç›‘æ§è£…é¥°å™¨"""
        def decorator(func):
            @wraps(func)
            async def async_wrapper(*args, **kwargs):
                start_time = time.time()
                start_memory = psutil.Process().memory_info().rss / 1024 / 1024
                
                try:
                    if asyncio.iscoroutinefunction(func):
                        result = await func(*args, **kwargs)
                    else:
                        result = func(*args, **kwargs)
                    
                    end_time = time.time()
                    end_memory = psutil.Process().memory_info().rss / 1024 / 1024
                    
                    # è®°å½•æ€§èƒ½æŒ‡æ ‡
                    execution_time = end_time - start_time
                    memory_delta = end_memory - start_memory
                    
                    self.record_metrics(node_name, execution_time, memory_delta)
                    
                    return result
                    
                except Exception as e:
                    self.logger.error(f"èŠ‚ç‚¹ {node_name} æ‰§è¡Œå¤±è´¥: {e}")
                    raise
            
            return async_wrapper
        return decorator
    
    def record_metrics(self, node_name: str, execution_time: float, memory_delta: float):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        if node_name not in self.metrics:
            self.metrics[node_name] = {
                "executions": 0,
                "total_time": 0,
                "max_time": 0,
                "min_time": float('inf'),
                "total_memory": 0,
                "max_memory": 0
            }
        
        metrics = self.metrics[node_name]
        metrics["executions"] += 1
        metrics["total_time"] += execution_time
        metrics["max_time"] = max(metrics["max_time"], execution_time)
        metrics["min_time"] = min(metrics["min_time"], execution_time)
        metrics["total_memory"] += memory_delta
        metrics["max_memory"] = max(metrics["max_memory"], memory_delta)
        
        # è®°å½•æ—¥å¿—
        avg_time = metrics["total_time"] / metrics["executions"]
        self.logger.info(
            f"èŠ‚ç‚¹ {node_name}: æ‰§è¡Œæ—¶é—´={execution_time:.3f}s, "
            f"å¹³å‡æ—¶é—´={avg_time:.3f}s, å†…å­˜å˜åŒ–={memory_delta:.1f}MB"
        )
    
    def get_performance_report(self) -> Dict:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        report = {}
        for node_name, metrics in self.metrics.items():
            if metrics["executions"] > 0:
                report[node_name] = {
                    "executions": metrics["executions"],
                    "avg_time": metrics["total_time"] / metrics["executions"],
                    "max_time": metrics["max_time"],
                    "min_time": metrics["min_time"],
                    "avg_memory": metrics["total_memory"] / metrics["executions"],
                    "max_memory": metrics["max_memory"]
                }
        return report

# å…¨å±€æ€§èƒ½ç›‘æ§å™¨
performance_monitor = PerformanceMonitor()

# ä½¿ç”¨ç¤ºä¾‹
@performance_monitor.monitor_node("ai_analysis")
async def monitored_ai_analysis(state: dict) -> dict:
    # AIåˆ†æé€»è¾‘
    pass
```

### 3. é”™è¯¯å¤„ç†ä¸é‡è¯•æœºåˆ¶

```python
import asyncio
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from typing import Optional, Callable, Any

class RobustNodeWrapper:
    """å¥å£®çš„èŠ‚ç‚¹åŒ…è£…å™¨"""
    
    @staticmethod
    def with_retry(
        max_attempts: int = 3,
        min_wait: float = 1,
        max_wait: float = 10,
        exceptions: tuple = (Exception,)
    ):
        """é‡è¯•è£…é¥°å™¨"""
        def decorator(func):
            @retry(
                stop=stop_after_attempt(max_attempts),
                wait=wait_exponential(multiplier=1, min=min_wait, max=max_wait),
                retry=retry_if_exception_type(exceptions)
            )
            @wraps(func)
            async def wrapper(*args, **kwargs):
                if asyncio.iscoroutinefunction(func):
                    return await func(*args, **kwargs)
                else:
                    return func(*args, **kwargs)
            return wrapper
        return decorator
    
    @staticmethod
    def with_fallback(fallback_func: Callable):
        """é™çº§å¤„ç†è£…é¥°å™¨"""
        def decorator(func):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                try:
                    if asyncio.iscoroutinefunction(func):
                        return await func(*args, **kwargs)
                    else:
                        return func(*args, **kwargs)
                except Exception as e:
                    logger.warning(f"ä¸»å‡½æ•°å¤±è´¥ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ: {e}")
                    if asyncio.iscoroutinefunction(fallback_func):
                        return await fallback_func(*args, **kwargs)
                    else:
                        return fallback_func(*args, **kwargs)
            return wrapper
        return decorator

# ä½¿ç”¨ç¤ºä¾‹
@RobustNodeWrapper.with_retry(max_attempts=3, exceptions=(ConnectionError, TimeoutError))
async def reliable_ai_call(state: dict) -> dict:
    """å¯é çš„AIè°ƒç”¨"""
    response = await llm.ainvoke(messages)
    return {"result": response.content}

def fallback_analysis(state: dict) -> dict:
    """é™çº§åˆ†ææ–¹æ¡ˆ"""
    return {"result": "ä½¿ç”¨è§„åˆ™åŸºç¡€çš„åˆ†æç»“æœ"}

@RobustNodeWrapper.with_fallback(fallback_analysis)
async def ai_with_fallback(state: dict) -> dict:
    """å¸¦é™çº§çš„AIåˆ†æ"""
    # å¯èƒ½å¤±è´¥çš„AIè°ƒç”¨
    response = await llm.ainvoke(messages)
    return {"result": response.content}
```

### 4. èµ„æºç®¡ç†ä¸é™æµ

```python
import asyncio
from asyncio import Semaphore
from typing import Dict, Any
import time

class ResourceManager:
    """èµ„æºç®¡ç†å™¨"""
    
    def __init__(self):
        self.semaphores: Dict[str, Semaphore] = {}
        self.rate_limiters: Dict[str, 'RateLimiter'] = {}
    
    def get_semaphore(self, resource: str, max_concurrent: int) -> Semaphore:
        """è·å–ä¿¡å·é‡"""
        if resource not in self.semaphores:
            self.semaphores[resource] = Semaphore(max_concurrent)
        return self.semaphores[resource]
    
    def get_rate_limiter(self, resource: str, max_calls: int, time_window: float) -> 'RateLimiter':
        """è·å–é™æµå™¨"""
        if resource not in self.rate_limiters:
            self.rate_limiters[resource] = RateLimiter(max_calls, time_window)
        return self.rate_limiters[resource]

class RateLimiter:
    """é€Ÿç‡é™åˆ¶å™¨"""
    
    def __init__(self, max_calls: int, time_window: float):
        self.max_calls = max_calls
        self.time_window = time_window
        self.calls = []
    
    async def acquire(self):
        """è·å–è°ƒç”¨è®¸å¯"""
        now = time.time()
        
        # æ¸…ç†è¿‡æœŸçš„è°ƒç”¨è®°å½•
        self.calls = [call_time for call_time in self.calls 
                     if now - call_time < self.time_window]
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
        if len(self.calls) >= self.max_calls:
            sleep_time = self.time_window - (now - self.calls[0])
            if sleep_time > 0:
                await asyncio.sleep(sleep_time)
        
        self.calls.append(now)

# å…¨å±€èµ„æºç®¡ç†å™¨
resource_manager = ResourceManager()

def with_concurrency_limit(resource: str, max_concurrent: int):
    """å¹¶å‘é™åˆ¶è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            semaphore = resource_manager.get_semaphore(resource, max_concurrent)
            async with semaphore:
                if asyncio.iscoroutinefunction(func):
                    return await func(*args, **kwargs)
                else:
                    return func(*args, **kwargs)
        return wrapper
    return decorator

def with_rate_limit(resource: str, max_calls: int, time_window: float):
    """é€Ÿç‡é™åˆ¶è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            rate_limiter = resource_manager.get_rate_limiter(resource, max_calls, time_window)
            await rate_limiter.acquire()
            
            if asyncio.iscoroutinefunction(func):
                return await func(*args, **kwargs)
            else:
                return func(*args, **kwargs)
        return wrapper
    return decorator

# ä½¿ç”¨ç¤ºä¾‹
@with_concurrency_limit("openai_api", max_concurrent=10)
@with_rate_limit("openai_api", max_calls=60, time_window=60.0)
async def rate_limited_ai_call(state: dict) -> dict:
    """é™æµçš„AIè°ƒç”¨"""
    response = await llm.ainvoke(messages)
    return {"result": response.content}
```

## å¸¸è§é”™è¯¯ä¸è§£å†³æ–¹æ¡ˆ

### 1. æ··åˆåŒæ­¥å¼‚æ­¥ä»£ç çš„é™·é˜±

```python
# âŒ å¸¸è§é”™è¯¯1ï¼šåœ¨å¼‚æ­¥å‡½æ•°ä¸­è°ƒç”¨åŒæ­¥é˜»å¡æ“ä½œ
async def bad_async_function():
    # è¿™ä¼šé˜»å¡æ•´ä¸ªäº‹ä»¶å¾ªç¯ï¼
    time.sleep(10)  
    result = requests.get("http://api.example.com")  # åŒæ­¥HTTPè°ƒç”¨
    return result

# âœ… æ­£ç¡®åšæ³•
async def good_async_function():
    # ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†é˜»å¡æ“ä½œ
    await asyncio.sleep(10)  # å¼‚æ­¥ç¡çœ 
    
    # ä½¿ç”¨å¼‚æ­¥HTTPå®¢æˆ·ç«¯
    async with aiohttp.ClientSession() as session:
        async with session.get("http://api.example.com") as response:
            return await response.json()

# âŒ å¸¸è§é”™è¯¯2ï¼šå¿˜è®°awaitå¼‚æ­¥è°ƒç”¨
async def bad_ai_call():
    result = llm.ainvoke(messages)  # å¿˜è®°awaitï¼
    return result  # è¿”å›çš„æ˜¯coroutineå¯¹è±¡ï¼Œä¸æ˜¯ç»“æœ

# âœ… æ­£ç¡®åšæ³•
async def good_ai_call():
    result = await llm.ainvoke(messages)  # æ­£ç¡®await
    return result
```

### 2. çŠ¶æ€ç®¡ç†é”™è¯¯

```python
# âŒ å¸¸è§é”™è¯¯ï¼šçŠ¶æ€é”®åä¸åŒ¹é…
class BadState(TypedDict):
    input_text: str
    result: str

def bad_node(state: BadState) -> dict:
    # è¿”å›çš„é”®åä¸TypedDictä¸åŒ¹é…
    return {"output": "processed"}  # åº”è¯¥æ˜¯"result"

# âœ… æ­£ç¡®åšæ³•
class GoodState(TypedDict):
    input_text: str
    result: str

def good_node(state: GoodState) -> dict:
    # è¿”å›æ­£ç¡®çš„é”®å
    return {"result": "processed"}

# âŒ å¸¸è§é”™è¯¯ï¼šä¿®æ”¹åŸå§‹çŠ¶æ€
def bad_state_modifier(state: dict) -> dict:
    state["new_field"] = "value"  # ç›´æ¥ä¿®æ”¹åŸå§‹çŠ¶æ€
    return state

# âœ… æ­£ç¡®åšæ³•
def good_state_modifier(state: dict) -> dict:
    # è¿”å›æ–°çš„çŠ¶æ€æ›´æ–°
    return {"new_field": "value"}
```

### 3. é”™è¯¯å¤„ç†ä¸å½“

```python
# âŒ å¸¸è§é”™è¯¯ï¼šä¸å¤„ç†å¼‚æ­¥å¼‚å¸¸
async def bad_error_handling():
    tasks = [risky_async_operation() for _ in range(10)]
    results = await asyncio.gather(*tasks)  # ä¸€ä¸ªå¤±è´¥å…¨éƒ¨å¤±è´¥
    return results

# âœ… æ­£ç¡®åšæ³•ï¼šä½¿ç”¨return_exceptions
async def good_error_handling():
    tasks = [risky_async_operation() for _ in range(10)]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # åˆ†åˆ«å¤„ç†æˆåŠŸå’Œå¤±è´¥çš„ç»“æœ
    successes = [r for r in results if not isinstance(r, Exception)]
    failures = [r for r in results if isinstance(r, Exception)]
    
    logger.info(f"æˆåŠŸ: {len(successes)}, å¤±è´¥: {len(failures)}")
    return successes
```

### 4. èµ„æºæ³„æ¼é—®é¢˜

```python
# âŒ å¸¸è§é”™è¯¯ï¼šä¸æ­£ç¡®å…³é—­èµ„æº
async def bad_resource_management():
    session = aiohttp.ClientSession()
    response = await session.get("http://api.example.com")
    return await response.json()  # å¿˜è®°å…³é—­sessionå’Œresponse

# âœ… æ­£ç¡®åšæ³•ï¼šä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨
async def good_resource_management():
    async with aiohttp.ClientSession() as session:
        async with session.get("http://api.example.com") as response:
            return await response.json()  # è‡ªåŠ¨å…³é—­èµ„æº
```

## ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æŒ‡å—

### 1. DockeråŒ–éƒ¨ç½²

```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£…Pythonä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/app
ENV WORKERS=4
ENV MAX_CONCURRENT=50

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  langgraph-app:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - REDIS_URL=redis://redis:6379
      - DATABASE_URL=postgresql://user:pass@postgres:5432/langgraph
    depends_on:
      - redis
      - postgres
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: langgraph
      POSTGRES_USER: user
      POSTGRES_PASSWORD: pass
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

volumes:
  postgres_data:
```

### 2. Kuberneteséƒ¨ç½²

```yaml
# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: langgraph-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: langgraph-app
  template:
    metadata:
      labels:
        app: langgraph-app
    spec:
      containers:
      - name: app
        image: langgraph-app:latest
        ports:
        - containerPort: 8000
        env:
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: api-secrets
              key: openai-key
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: langgraph-service
spec:
  selector:
    app: langgraph-app
  ports:
  - port: 80
    targetPort: 8000
  type: LoadBalancer
```

### 3. ç›‘æ§ä¸æ—¥å¿—

```python
# monitoring.py
import logging
import time
from prometheus_client import Counter, Histogram, Gauge, start_http_server
from functools import wraps

# PrometheusæŒ‡æ ‡
REQUEST_COUNT = Counter('langgraph_requests_total', 'Total requests', ['method', 'endpoint', 'status'])
REQUEST_DURATION = Histogram('langgraph_request_duration_seconds', 'Request duration')
ACTIVE_CONNECTIONS = Gauge('langgraph_active_connections', 'Active connections')
NODE_EXECUTION_TIME = Histogram('langgraph_node_execution_seconds', 'Node execution time', ['node_name'])

class ProductionMonitor:
    """ç”Ÿäº§ç¯å¢ƒç›‘æ§"""
    
    def __init__(self):
        self.logger = self.setup_logging()
        # å¯åŠ¨PrometheusæŒ‡æ ‡æœåŠ¡å™¨
        start_http_server(9090)
    
    def setup_logging(self):
        """è®¾ç½®ç»“æ„åŒ–æ—¥å¿—"""
        import json_logging
        import sys
        
        json_logging.init_fastapi(enable_json=True)
        json_logging.init_request_instrument(app)
        
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.INFO)
        
        # æ·»åŠ æ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler('/var/log/langgraph/app.log')
        file_handler.setLevel(logging.INFO)
        
        # æ·»åŠ æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(logging.INFO)
        
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
        
        return logger
    
    def monitor_request(self, func):
        """è¯·æ±‚ç›‘æ§è£…é¥°å™¨"""
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            ACTIVE_CONNECTIONS.inc()
            
            try:
                result = await func(*args, **kwargs)
                REQUEST_COUNT.labels(method='POST', endpoint='/analyze', status='success').inc()
                return result
            except Exception as e:
                REQUEST_COUNT.labels(method='POST', endpoint='/analyze', status='error').inc()
                self.logger.error(f"è¯·æ±‚å¤„ç†å¤±è´¥: {e}", exc_info=True)
                raise
            finally:
                REQUEST_DURATION.observe(time.time() - start_time)
                ACTIVE_CONNECTIONS.dec()
        
        return wrapper
    
    def monitor_node(self, node_name: str):
        """èŠ‚ç‚¹ç›‘æ§è£…é¥°å™¨"""
        def decorator(func):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                start_time = time.time()
                
                try:
                    if asyncio.iscoroutinefunction(func):
                        result = await func(*args, **kwargs)
                    else:
                        result = func(*args, **kwargs)
                    
                    NODE_EXECUTION_TIME.labels(node_name=node_name).observe(time.time() - start_time)
                    
                    self.logger.info(
                        f"èŠ‚ç‚¹æ‰§è¡Œå®Œæˆ",
                        extra={
                            "node_name": node_name,
                            "execution_time": time.time() - start_time,
                            "status": "success"
                        }
                    )
                    
                    return result
                    
                except Exception as e:
                    self.logger.error(
                        f"èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥",
                        extra={
                            "node_name": node_name,
                            "execution_time": time.time() - start_time,
                            "status": "error",
                            "error": str(e)
                        },
                        exc_info=True
                    )
                    raise
            
            return wrapper
        return decorator

# å…¨å±€ç›‘æ§å™¨
production_monitor = ProductionMonitor()
```

### 4. é…ç½®ç®¡ç†

```python
# config.py
import os
from pydantic import BaseSettings
from typing import Optional

class Settings(BaseSettings):
    """åº”ç”¨é…ç½®"""
    
    # APIé…ç½®
    openai_api_key: str
    openai_model: str = "gpt-4"
    openai_temperature: float = 0.0
    openai_max_retries: int = 3
    
    # åº”ç”¨é…ç½®
    app_name: str = "LangGraphæ–‡æœ¬åˆ†æå™¨"
    app_version: str = "2.0.0"
    debug: bool = False
    
    # æœåŠ¡å™¨é…ç½®
    host: str = "0.0.0.0"
    port: int = 8000
    workers: int = 4
    
    # æ€§èƒ½é…ç½®
    max_concurrent_requests: int = 100
    request_timeout: float = 300.0
    thread_pool_size: int = 16
    
    # æ•°æ®åº“é…ç½®
    database_url: Optional[str] = None
    redis_url: Optional[str] = None
    
    # ç›‘æ§é…ç½®
    enable_metrics: bool = True
    metrics_port: int = 9090
    log_level: str = "INFO"
    
    # é™æµé…ç½®
    rate_limit_calls: int = 100
    rate_limit_window: float = 60.0
    
    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"

settings = Settings()
```

### 5. å¥åº·æ£€æŸ¥ä¸ä¼˜é›…å…³é—­

```python
# health.py
import asyncio
import signal
import sys
from contextlib import asynccontextmanager
from fastapi import FastAPI

class HealthChecker:
    """å¥åº·æ£€æŸ¥å™¨"""
    
    def __init__(self):
        self.is_healthy = True
        self.dependencies = {}
    
    async def check_openai_api(self) -> bool:
        """æ£€æŸ¥OpenAI APIè¿æ¥"""
        try:
            # ç®€å•çš„APIæµ‹è¯•
            response = await llm.ainvoke([HumanMessage(content="test")])
            return True
        except Exception as e:
            logger.error(f"OpenAI APIæ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    async def check_database(self) -> bool:
        """æ£€æŸ¥æ•°æ®åº“è¿æ¥"""
        if not settings.database_url:
            return True
        
        try:
            # æ•°æ®åº“è¿æ¥æ£€æŸ¥é€»è¾‘
            return True
        except Exception as e:
            logger.error(f"æ•°æ®åº“æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    async def check_redis(self) -> bool:
        """æ£€æŸ¥Redisè¿æ¥"""
        if not settings.redis_url:
            return True
        
        try:
            # Redisè¿æ¥æ£€æŸ¥é€»è¾‘
            return True
        except Exception as e:
            logger.error(f"Redisæ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    async def comprehensive_health_check(self) -> dict:
        """ç»¼åˆå¥åº·æ£€æŸ¥"""
        checks = {
            "openai_api": await self.check_openai_api(),
            "database": await self.check_database(),
            "redis": await self.check_redis()
        }
        
        overall_health = all(checks.values())
        
        return {
            "status": "healthy" if overall_health else "unhealthy",
            "checks": checks,
            "timestamp": time.time()
        }

health_checker = HealthChecker()

class GracefulShutdown:
    """ä¼˜é›…å…³é—­å¤„ç†å™¨"""
    
    def __init__(self):
        self.shutdown_event = asyncio.Event()
        self.setup_signal_handlers()
    
    def setup_signal_handlers(self):
        """è®¾ç½®ä¿¡å·å¤„ç†å™¨"""
        for sig in (signal.SIGTERM, signal.SIGINT):
            signal.signal(sig, self.signal_handler)
    
    def signal_handler(self, signum, frame):
        """ä¿¡å·å¤„ç†å‡½æ•°"""
        logger.info(f"æ”¶åˆ°ä¿¡å· {signum}ï¼Œå¼€å§‹ä¼˜é›…å…³é—­...")
        self.shutdown_event.set()
    
    async def shutdown_sequence(self):
        """å…³é—­åºåˆ—"""
        logger.info("å¼€å§‹å…³é—­åºåˆ—...")
        
        # 1. åœæ­¢æ¥æ”¶æ–°è¯·æ±‚
        logger.info("åœæ­¢æ¥æ”¶æ–°è¯·æ±‚")
        
        # 2. ç­‰å¾…ç°æœ‰è¯·æ±‚å®Œæˆ
        logger.info("ç­‰å¾…ç°æœ‰è¯·æ±‚å®Œæˆ...")
        await asyncio.sleep(5)  # ç»™ç°æœ‰è¯·æ±‚æ—¶é—´å®Œæˆ
        
        # 3. å…³é—­çº¿ç¨‹æ± 
        if hasattr(async_wrapper, 'executor'):
            logger.info("å…³é—­çº¿ç¨‹æ± ...")
            async_wrapper.executor.shutdown(wait=True)
        
        # 4. å…³é—­æ•°æ®åº“è¿æ¥
        logger.info("å…³é—­æ•°æ®åº“è¿æ¥...")
        
        # 5. æ¸…ç†å…¶ä»–èµ„æº
        logger.info("æ¸…ç†å…¶ä»–èµ„æº...")
        
        logger.info("ä¼˜é›…å…³é—­å®Œæˆ")

graceful_shutdown = GracefulShutdown()

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶æ‰§è¡Œ
    logger.info("åº”ç”¨å¯åŠ¨ä¸­...")
    
    # åˆå§‹åŒ–æ£€æŸ¥
    health_status = await health_checker.comprehensive_health_check()
    if health_status["status"] != "healthy":
        logger.error("åº”ç”¨å¯åŠ¨å¤±è´¥ï¼Œå¥åº·æ£€æŸ¥æœªé€šè¿‡")
        sys.exit(1)
    
    logger.info("åº”ç”¨å¯åŠ¨å®Œæˆ")
    
    yield
    
    # å…³é—­æ—¶æ‰§è¡Œ
    await graceful_shutdown.shutdown_sequence()

# åœ¨FastAPIåº”ç”¨ä¸­ä½¿ç”¨
app = FastAPI(lifespan=lifespan)

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return await health_checker.comprehensive_health_check()

@app.get("/health/live")
async def liveness_probe():
    """å­˜æ´»æ¢é’ˆ"""
    return {"status": "alive", "timestamp": time.time()}

@app.get("/health/ready")
async def readiness_probe():
    """å°±ç»ªæ¢é’ˆ"""
    health_status = await health_checker.comprehensive_health_check()
    if health_status["status"] == "healthy":
        return health_status
    else:
        raise HTTPException(status_code=503, detail=health_status)
```

## æ€»ç»“

### å…³é”®æ”¶ç›Šå›é¡¾

é€šè¿‡é‡‡ç”¨å¼‚æ­¥LangGraphå’Œæ··åˆèŠ‚ç‚¹æ¶æ„ï¼Œæˆ‘ä»¬å®ç°äº†ï¼š

| æ€§èƒ½æŒ‡æ ‡ | æ”¹è¿›å‰ | æ”¹è¿›å | æå‡å¹…åº¦ |
|:---|:---:|:---:|:---:|
| **å“åº”æ—¶é—´** | 30ç§’ | 3ç§’ | **90%å‡å°‘** |
| **å¹¶å‘å¤„ç†** | 1ä¸ªç”¨æˆ· | 50ä¸ªç”¨æˆ· | **5000%æå‡** |
| **èµ„æºåˆ©ç”¨ç‡** | 20% | 85% | **325%æå‡** |
| **é”™è¯¯ç‡** | 15% | 2% | **87%å‡å°‘** |
| **æˆæœ¬æ•ˆç‡** | åŸºå‡† | èŠ‚çœ60% | **æ˜¾è‘—é™ä½** |

### æœ€ä½³å®è·µæ€»ç»“

```mermaid
mindmap
  root((LangGraphå¼‚æ­¥æœ€ä½³å®è·µ))
    èŠ‚ç‚¹è®¾è®¡
      åŒæ­¥èŠ‚ç‚¹
        æ•°æ®éªŒè¯
        ç®€å•è®¡ç®—
        é…ç½®è¯»å–
      å¼‚æ­¥èŠ‚ç‚¹
        AI APIè°ƒç”¨
        HTTPè¯·æ±‚
        æ•°æ®åº“æ“ä½œ
      çº¿ç¨‹æ± èŠ‚ç‚¹
        CPUå¯†é›†å‹
        åŒæ­¥åº“è°ƒç”¨
        æ–‡ä»¶å¤„ç†
    
    æ€§èƒ½ä¼˜åŒ–
      å¹¶å‘æ§åˆ¶
        ä¿¡å·é‡é™åˆ¶
        é€Ÿç‡é™åˆ¶
        èµ„æºæ± ç®¡ç†
      ç›‘æ§å‘Šè­¦
        æ€§èƒ½æŒ‡æ ‡
        é”™è¯¯è¿½è¸ª
        å¥åº·æ£€æŸ¥
      é”™è¯¯å¤„ç†
        é‡è¯•æœºåˆ¶
        é™çº§æ–¹æ¡ˆ
        å¼‚å¸¸æ¢å¤
    
    ç”Ÿäº§éƒ¨ç½²
      å®¹å™¨åŒ–
        Dockeré•œåƒ
        K8séƒ¨ç½²
        é…ç½®ç®¡ç†
      ç›‘æ§è¿ç»´
        æ—¥å¿—èšåˆ
        æŒ‡æ ‡æ”¶é›†
        å‘Šè­¦é€šçŸ¥
      æ‰©å±•æ€§
        æ°´å¹³æ‰©å±•
        è´Ÿè½½å‡è¡¡
        ç¼“å­˜ç­–ç•¥
```

### æ ¸å¿ƒè¦ç‚¹

1. **æ··åˆèŠ‚ç‚¹æ¶æ„**ï¼šLangGraphåŸç”Ÿæ”¯æŒåŒæ­¥å’Œå¼‚æ­¥èŠ‚ç‚¹æ··åˆä½¿ç”¨ï¼Œæ— éœ€å…¨éƒ¨æ”¹å†™ä¸ºå¼‚æ­¥
2. **çº¿ç¨‹æ± åŒ…è£…**ï¼šå¯¹äºä¸æ”¯æŒå¼‚æ­¥çš„è€—æ—¶æ“ä½œï¼Œä½¿ç”¨`asyncio.to_thread()`æˆ–çº¿ç¨‹æ± åŒ…è£…
3. **æ€§èƒ½ç›‘æ§**ï¼šå®æ–½å…¨é¢çš„æ€§èƒ½ç›‘æ§å’Œé”™è¯¯å¤„ç†æœºåˆ¶
4. **èµ„æºç®¡ç†**ï¼šåˆç†æ§åˆ¶å¹¶å‘æ•°é‡å’Œèµ„æºä½¿ç”¨ï¼Œé¿å…ç³»ç»Ÿè¿‡è½½
5. **ç”Ÿäº§å°±ç»ª**ï¼šè€ƒè™‘å¥åº·æ£€æŸ¥ã€ä¼˜é›…å…³é—­ã€ç›‘æ§å‘Šè­¦ç­‰ç”Ÿäº§ç¯å¢ƒéœ€æ±‚

### ä¸‹ä¸€æ­¥è¡ŒåŠ¨

å¦‚æœä½ æ­£åœ¨æ„å»ºéœ€è¦å¤„ç†å¤šç”¨æˆ·å¹¶å‘çš„AIåº”ç”¨ï¼š

1. **ç«‹å³å¼€å§‹**ï¼šä»ç®€å•çš„å¼‚æ­¥èŠ‚ç‚¹å¼€å§‹ï¼Œé€æ­¥è¿ç§»ç°æœ‰ä»£ç 
2. **æ€§èƒ½æµ‹è¯•**ï¼šåœ¨ç±»ä¼¼ç”Ÿäº§ç¯å¢ƒçš„è´Ÿè½½ä¸‹æµ‹è¯•ä½ çš„åº”ç”¨
3. **ç›‘æ§éƒ¨ç½²**ï¼šå®æ–½å…¨é¢çš„ç›‘æ§å’Œå‘Šè­¦æœºåˆ¶
4. **æŒç»­ä¼˜åŒ–**ï¼šåŸºäºå®é™…ä½¿ç”¨æ•°æ®æŒç»­ä¼˜åŒ–æ€§èƒ½

ä¸€æ—¦ä½ ä½“éªŒåˆ°å“åº”æ—¶é—´ä»30ç§’é™åˆ°3ç§’çš„å¿«æ„Ÿï¼Œä½ å°±å†ä¹Ÿä¸ä¼šå›åˆ°åŒæ­¥AIå·¥ä½œæµç¨‹äº†ã€‚

---

